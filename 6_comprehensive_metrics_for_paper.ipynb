{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fae0d8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a875dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\py310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6f177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import jinja2\n",
    "import statsmodels\n",
    "warnings.filterwarnings('ignore')\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    cohen_kappa_score, matthews_corrcoef,\n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import sem, t as t_dist\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.precision', 4)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dc346",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f7723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Metrics output directory: C:\\Users\\Nguyen Ngo\\Downloads\\NLP PApere\\NLP-Trade-Message\\metrics_tables\n",
      "‚úÖ Loaded dataset: 1,334 records\n",
      "‚úÖ Classes (5): drug, fraud, guide, hacking, other\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path(\"./data\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "BASELINE_DIR = MODELS_DIR \n",
    "BERT_MODELS_DIR = MODELS_DIR / \"bert_models\"\n",
    "OUTPUT_DIR = Path(\"./outputs\")\n",
    "METRICS_DIR = Path(\"./metrics_tables\")\n",
    "METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"üìÇ Metrics output directory: {METRICS_DIR.resolve()}\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_DIR / \"english_clean.csv\")\n",
    "print(f\"‚úÖ Loaded dataset: {len(df):,} records\")\n",
    "\n",
    "# Load label encoder\n",
    "with open(BERT_MODELS_DIR / 'label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "classes = label_encoder.classes_\n",
    "n_classes = len(classes)\n",
    "print(f\"‚úÖ Classes ({n_classes}): {', '.join(classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083dc218",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Data (Same Split as Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9003e963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Split:\n",
      "   Training: 933 samples\n",
      "   Validation: 200 samples\n",
      "   Test: 201 samples\n",
      "\n",
      "‚úÖ Test set ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "text_column = 'combined_text' if 'combined_text' in df.columns else 'clean_text'\n",
    "X = df[text_column].values\n",
    "y = label_encoder.transform(df['label'].values)\n",
    "\n",
    "# Split (same as training notebooks)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"   Training: {len(X_train):,} samples\")\n",
    "print(f\"   Validation: {len(X_val):,} samples\")\n",
    "print(f\"   Test: {len(X_test):,} samples\")\n",
    "print(f\"\\n‚úÖ Test set ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8490870",
   "metadata": {},
   "source": [
    "## 4. Load All Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb083458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING PREDICTIONS FOR ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "üîÑ Logistic Regression...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Svm...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Random Forest...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ Gradient Boosting...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ BERT...\n",
      "üîÑ Generating predictions for BERT...\n",
      "   Model dir: models\\bert_models\\bert_seed42_1769880355_final\n",
      "   Device: cuda\n",
      "   ‚úÖ Done\n",
      "\n",
      "üîÑ ROBERTA...\n",
      "üîÑ Generating predictions for ROBERTA...\n",
      "   Model dir: models\\bert_models\\roberta_seed42_1769881070_final\n",
      "   Device: cuda\n",
      "   ‚úÖ Done\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All predictions generated! Total models: 6\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# NOTE: If you see Windows error 1455 (paging file too small) when loading model.safetensors,\n",
    "# increase your Windows paging file or temporarily evaluate fewer transformer models (e.g., only RoBERTa).\n",
    "\n",
    "# Why you currently only see RoBERTa: SAFE_MODE defaults to RoBERTa-only to avoid Windows paging-file / RAM crashes.\n",
    "# Set SAFE_MODE=False to evaluate both BERT + RoBERTa in the same run.\n",
    "SAFE_MODE = False  # True = safer (RoBERTa only), False = run both transformers\n",
    "\n",
    "if SAFE_MODE:\n",
    "    TRANSFORMER_MODEL_KEYS = ['roberta']\n",
    "else:\n",
    "    TRANSFORMER_MODEL_KEYS = ['bert', 'roberta']\n",
    "\n",
    "TRANSFORMER_PREFER_PROFILE = 'COMPARE'  # None / 'FAST' / 'COMPARE' / 'PAPER'\n",
    "TRANSFORMER_BATCH_SIZE = 16\n",
    "TRANSFORMER_MAX_LENGTH = 128\n",
    "\n",
    "def get_baseline_predictions(model_name):\n",
    "    \"\"\"\n",
    "    Get predictions from baseline models\n",
    "    \"\"\"\n",
    "    # Load TF-IDF vectorizer\n",
    "    with open(BASELINE_DIR / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    \n",
    "    # Load all baseline models\n",
    "    with open(BASELINE_DIR / 'all_baseline_models.pkl', 'rb') as f:\n",
    "        all_models = pickle.load(f)\n",
    "    \n",
    "    # Map display names to saved model names\n",
    "    model_name_map = {\n",
    "        'logistic_regression': 'Logistic Regression',\n",
    "        'svm': 'Linear SVM',\n",
    "        'random_forest': 'Random Forest',\n",
    "        'gradient_boosting': 'Gradient Boosting'\n",
    "    }\n",
    "    \n",
    "    # Get model\n",
    "    saved_model_name = model_name_map.get(model_name, model_name)\n",
    "    if saved_model_name not in all_models:\n",
    "        print(f\"‚ö†Ô∏è  Model '{saved_model_name}' not found in all_baseline_models.pkl\")\n",
    "        print(f\"   Available models: {list(all_models.keys())}\")\n",
    "        return None, None\n",
    "    \n",
    "    model = all_models[saved_model_name]\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Ensure predictions are integers (not strings)\n",
    "    if y_pred.dtype == 'object' or y_pred.dtype.kind in ['U', 'S']:\n",
    "        # If predictions are strings, convert using label encoder\n",
    "        y_pred = label_encoder.transform(y_pred)\n",
    "    else:\n",
    "        y_pred = y_pred.astype(int)\n",
    "    \n",
    "    # Probabilities (if available)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_tfidf)\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        # For SVM\n",
    "        y_proba = model.decision_function(X_test_tfidf)\n",
    "        # Normalize to [0, 1]\n",
    "        from sklearn.preprocessing import minmax_scale\n",
    "        y_proba = minmax_scale(y_proba, axis=1)\n",
    "    else:\n",
    "        y_proba = None\n",
    "    \n",
    "    return y_pred, y_proba\n",
    "\n",
    "def _read_transformer_runs_summary(path):\n",
    "    \"\"\"Robust CSV loader: handles older rows (15 cols) and newer rows (16 cols with run_profile).\"\"\"\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        header = [h.strip() for h in header]\n",
    "        rows = [r for r in reader if r]\n",
    "\n",
    "    header_len = len(header)\n",
    "    row_lens = [len(r) for r in rows]\n",
    "    max_len = max(row_lens) if row_lens else header_len\n",
    "\n",
    "    if max_len == header_len + 1:\n",
    "        # Newer rows include run_profile between checkpoint and seed:\n",
    "        fixed_header = header[:3] + [\"run_profile\"] + header[3:]\n",
    "        fixed_rows = []\n",
    "        for r in rows:\n",
    "            if len(r) == header_len:\n",
    "                fixed_rows.append(r[:3] + [None] + r[3:])\n",
    "            elif len(r) == header_len + 1:\n",
    "                fixed_rows.append(r)\n",
    "        return pd.DataFrame(fixed_rows, columns=fixed_header)\n",
    "\n",
    "    # Best-effort: pad/truncate rows to header length\n",
    "    fixed_rows = []\n",
    "    for r in rows:\n",
    "        if len(r) < header_len:\n",
    "            fixed_rows.append(r + [None] * (header_len - len(r)))\n",
    "        else:\n",
    "            fixed_rows.append(r[:header_len])\n",
    "    return pd.DataFrame(fixed_rows, columns=header)\n",
    "\n",
    "def _resolve_model_dir_from_summary(model_key: str, prefer_profile: str | None = None):\n",
    "    \"\"\"Return Path to best run's final_model_dir for a given model key (e.g. 'bert', 'roberta').\"\"\"\n",
    "    summary_csv = BERT_MODELS_DIR / \"transformer_runs_summary.csv\"\n",
    "    if not summary_csv.exists():\n",
    "        return BERT_MODELS_DIR / f\"{model_key}_final\"\n",
    "\n",
    "    df_runs = _read_transformer_runs_summary(summary_csv)\n",
    "    if \"eval_f1\" not in df_runs.columns:\n",
    "        return BERT_MODELS_DIR / f\"{model_key}_final\"\n",
    "\n",
    "    # Normalize columns\n",
    "    df_runs[\"eval_f1\"] = pd.to_numeric(df_runs[\"eval_f1\"], errors=\"coerce\")\n",
    "    if \"model_name\" not in df_runs.columns or \"final_model_dir\" not in df_runs.columns:\n",
    "        return BERT_MODELS_DIR / f\"{model_key}_final\"\n",
    "\n",
    "    # Match model_name by common aliases\n",
    "    aliases = {\n",
    "        \"bert\": [\"bert\", \"BERT\"],\n",
    "        \"roberta\": [\"roberta\", \"RoBERTa\"],\n",
    "    }\n",
    "    candidates = aliases.get(model_key.lower(), [model_key])\n",
    "    m = df_runs[\"model_name\"].astype(str).str.lower().isin([c.lower() for c in candidates])\n",
    "    df_m = df_runs[m].copy()\n",
    "\n",
    "    if df_m.empty:\n",
    "        return BERT_MODELS_DIR / f\"{model_key}_final\"\n",
    "\n",
    "    if prefer_profile and \"run_profile\" in df_m.columns:\n",
    "        df_pref = df_m[df_m[\"run_profile\"].astype(str).str.upper() == str(prefer_profile).upper()]\n",
    "        if not df_pref.empty:\n",
    "            df_m = df_pref\n",
    "\n",
    "    best_row = df_m.sort_values([\"eval_f1\", \"run_tag\"], ascending=[False, False]).iloc[0]\n",
    "    final_dir = str(best_row[\"final_model_dir\"])\n",
    "    # Handle stored paths like 'models/bert_models/<run>_final' or '<run>_final'\n",
    "    p = Path(final_dir)\n",
    "    if p.is_absolute():\n",
    "        return p\n",
    "    final_dir_norm = final_dir.replace(\"\\\\\", \"/\")\n",
    "    if final_dir_norm.startswith(\"models/\"):\n",
    "        return Path(final_dir)\n",
    "    return BERT_MODELS_DIR / p\n",
    "\n",
    "def get_transformer_predictions(model_name='roberta', batch_size=16, max_length=128, prefer_profile=None):\n",
    "    \"\"\"\n",
    "    Get predictions from transformer models (batched inference to avoid kernel crashes).\n",
    "    \"\"\"\n",
    "    import os\n",
    "    # Try to avoid memory-mapped safetensors on Windows if the environment supports it.\n",
    "    os.environ.setdefault(\"SAFETENSORS_NO_MMAP\", \"1\")\n",
    "\n",
    "    model_dir = _resolve_model_dir_from_summary(model_name, prefer_profile=prefer_profile)\n",
    "\n",
    "    if not model_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è  Model not found: {model_dir}\")\n",
    "        return None, None\n",
    "\n",
    "    # Normalize texts (avoid NaNs)\n",
    "    texts = [\"\" if pd.isna(t) else str(t) for t in X_test]\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(model_dir))\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(str(model_dir))\n",
    "    except OSError as e:\n",
    "        msg = str(e)\n",
    "        winerr = getattr(e, \"winerror\", None)\n",
    "        if winerr == 1455 or \"paging file is too small\" in msg.lower() or \"os error 1455\" in msg.lower():\n",
    "            print(\"‚ö†Ô∏è  Windows paging file error while loading model weights (1455).\")\n",
    "            print(\"   Fix options:\")\n",
    "            print(\"   1) Increase Windows paging file size (recommended), then restart kernel\")\n",
    "            print(\"   2) Close other RAM-heavy apps, restart kernel\")\n",
    "            print(\"   3) Evaluate fewer transformer models (e.g., TRANSFORMER_MODEL_KEYS=['roberta'])\")\n",
    "            print(f\"   Failed model dir: {model_dir}\")\n",
    "            return None, None\n",
    "        raise\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"üîÑ Generating predictions for {model_name.upper()}...\")\n",
    "    print(f\"   Model dir: {model_dir}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "\n",
    "    def _infer_on_device(dev: torch.device):\n",
    "        y_pred_chunks = []\n",
    "        y_proba_chunks = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            enc = {k: v.to(dev) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                logits = model(**enc).logits\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "            y_pred_chunks.append(probs.argmax(dim=-1).detach().cpu().numpy())\n",
    "            y_proba_chunks.append(probs.detach().cpu().numpy())\n",
    "        return np.concatenate(y_pred_chunks), np.concatenate(y_proba_chunks)\n",
    "\n",
    "    try:\n",
    "        y_pred, y_proba = _infer_on_device(device)\n",
    "    except RuntimeError as e:\n",
    "        # Common kernel-crash precursor: CUDA OOM. Fall back to CPU.\n",
    "        if \"CUDA out of memory\" in str(e) and device.type == \"cuda\":\n",
    "            print(\"‚ö†Ô∏è  CUDA OOM during inference; retrying on CPU...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            device = torch.device('cpu')\n",
    "            model.to(device)\n",
    "            y_pred, y_proba = _infer_on_device(device)\n",
    "        else:\n",
    "            raise\n",
    "    finally:\n",
    "        # Free memory between models\n",
    "        del model\n",
    "        del tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return y_pred, y_proba\n",
    "\n",
    "# Generate predictions for all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS FOR ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "# Baseline models\n",
    "baseline_models = ['logistic_regression', 'svm', 'random_forest', 'gradient_boosting']\n",
    "for model_name in baseline_models:\n",
    "    print(f\"\\nüîÑ {model_name.replace('_', ' ').title()}...\")\n",
    "    y_pred, y_proba = get_baseline_predictions(model_name)\n",
    "    predictions[model_name] = y_pred\n",
    "    probabilities[model_name] = y_proba\n",
    "    print(\"   ‚úÖ Done\")\n",
    "\n",
    "# Transformer models (best run per model from transformer_runs_summary.csv if present)\n",
    "for model_name in TRANSFORMER_MODEL_KEYS:\n",
    "    print(f\"\\nüîÑ {model_name.upper()}...\")\n",
    "    y_pred, y_proba = get_transformer_predictions(\n",
    "        model_name,\n",
    "        batch_size=TRANSFORMER_BATCH_SIZE,\n",
    "        max_length=TRANSFORMER_MAX_LENGTH,\n",
    "        prefer_profile=TRANSFORMER_PREFER_PROFILE,\n",
    "    )\n",
    "    if y_pred is not None:\n",
    "        predictions[model_name] = y_pred\n",
    "        probabilities[model_name] = y_proba\n",
    "        print(\"   ‚úÖ Done\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ All predictions generated! Total models: {len(predictions)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bfe159",
   "metadata": {},
   "source": [
    "## 5. Calculate Comprehensive Metrics for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e93625c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CALCULATING COMPREHENSIVE METRICS\n",
      "================================================================================\n",
      "\n",
      "üìä Calculating metrics for Logistic Regression...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for SVM...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for Random Forest...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for Gradient Boosting...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for BERT...\n",
      "   ‚úÖ Done\n",
      "\n",
      "üìä Calculating metrics for RoBERTa...\n",
      "   ‚úÖ Done\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All metrics calculated!\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY (Top 5 Metrics):\n",
      "              Model  Accuracy  F1_Macro  Cohen_Kappa    MCC  ROC_AUC_Macro\n",
      "Logistic Regression    0.4925    0.1738       0.0823 0.1816         0.6238\n",
      "                SVM    0.5075    0.1880       0.1129 0.2137         0.5096\n",
      "      Random Forest    0.4627    0.1369       0.0202 0.0970         0.6078\n",
      "  Gradient Boosting    0.4478    0.1467       0.0061 0.0123         0.4977\n",
      "               BERT    0.8756    0.6968       0.8096 0.8105         0.9649\n",
      "            RoBERTa    0.9055    0.7400       0.8562 0.8570         0.9656\n"
     ]
    }
   ],
   "source": [
    "def calculate_confidence_interval(metric_values, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for a metric\n",
    "    \"\"\"\n",
    "    n = len(metric_values)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    mean = np.mean(metric_values)\n",
    "    std_err = sem(metric_values)\n",
    "    margin = std_err * t_dist.ppf((1 + confidence) / 2, n - 1)\n",
    "    \n",
    "    return margin\n",
    "\n",
    "def calculate_all_metrics(y_true, y_pred, y_proba=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for a model\n",
    "    \"\"\"\n",
    "    results = {'Model': model_name}\n",
    "    \n",
    "    # 1. Basic Metrics\n",
    "    results['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # 2. Precision, Recall, F1 (Macro, Micro, Weighted)\n",
    "    results['Precision_Macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    results['Precision_Micro'] = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    results['Precision_Weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    results['Recall_Macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    results['Recall_Micro'] = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    results['Recall_Weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    results['F1_Macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    results['F1_Micro'] = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    results['F1_Weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # 3. Cohen's Kappa\n",
    "    results['Cohen_Kappa'] = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # 4. Matthews Correlation Coefficient\n",
    "    results['MCC'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # 5. Confusion Matrix Statistics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate per-class TPR, FPR, TNR, FNR\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    tnr_list = []\n",
    "    fnr_list = []\n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        tp = cm[i, i]\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        tn = cm.sum() - tp - fn - fp\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity/Recall\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # Fall-out\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss rate\n",
    "        \n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "        tnr_list.append(tnr)\n",
    "        fnr_list.append(fnr)\n",
    "    \n",
    "    results['Avg_TPR'] = np.mean(tpr_list)\n",
    "    results['Avg_FPR'] = np.mean(fpr_list)\n",
    "    results['Avg_TNR'] = np.mean(tnr_list)\n",
    "    results['Avg_FNR'] = np.mean(fnr_list)\n",
    "    \n",
    "    # 6. Error Rate\n",
    "    results['Error_Rate'] = 1 - results['Accuracy']\n",
    "    \n",
    "    # 7. ROC-AUC (One-vs-Rest) if probabilities available\n",
    "    if y_proba is not None and len(y_proba.shape) == 2:\n",
    "        try:\n",
    "            # Binarize labels for multi-class\n",
    "            y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "            \n",
    "            # Calculate AUC for each class\n",
    "            auc_scores = []\n",
    "            for i in range(n_classes):\n",
    "                if len(np.unique(y_true_bin[:, i])) > 1:  # Need both classes\n",
    "                    auc_score = roc_auc_score(y_true_bin[:, i], y_proba[:, i])\n",
    "                    auc_scores.append(auc_score)\n",
    "            \n",
    "            results['ROC_AUC_Macro'] = np.mean(auc_scores) if auc_scores else 0.0\n",
    "            results['ROC_AUC_Weighted'] = roc_auc_score(\n",
    "                y_true_bin, y_proba, average='weighted', multi_class='ovr'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            results['ROC_AUC_Macro'] = np.nan\n",
    "            results['ROC_AUC_Weighted'] = np.nan\n",
    "    else:\n",
    "        results['ROC_AUC_Macro'] = np.nan\n",
    "        results['ROC_AUC_Weighted'] = np.nan\n",
    "    \n",
    "    # 8. Confidence Intervals (Bootstrap)\n",
    "    # Use per-sample correctness for CI\n",
    "    correct = (y_true == y_pred).astype(float)\n",
    "    accuracy_ci = calculate_confidence_interval(correct)\n",
    "    results['Accuracy_CI'] = accuracy_ci\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate metrics for all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "model_display_names = {\n",
    "    'logistic_regression': 'Logistic Regression',\n",
    "    'svm': 'SVM',\n",
    "    'random_forest': 'Random Forest',\n",
    "    'gradient_boosting': 'Gradient Boosting',\n",
    "    'bert': 'BERT',\n",
    "    'roberta': 'RoBERTa'\n",
    "}\n",
    "\n",
    "for model_key, y_pred in predictions.items():\n",
    "    model_name = model_display_names[model_key]\n",
    "    y_proba = probabilities.get(model_key)\n",
    "    \n",
    "    print(f\"\\nüìä Calculating metrics for {model_name}...\")\n",
    "    metrics = calculate_all_metrics(y_test, y_pred, y_proba, model_name)\n",
    "    all_metrics.append(metrics)\n",
    "    print(f\"   ‚úÖ Done\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(all_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All metrics calculated!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìä SUMMARY (Top 5 Metrics):\")\n",
    "print(df_metrics[['Model', 'Accuracy', 'F1_Macro', 'Cohen_Kappa', 'MCC', 'ROC_AUC_Macro']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6e019",
   "metadata": {},
   "source": [
    "## 6. Table 1: Standard Performance Metrics\n",
    "\n",
    "Main table for paper Results section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419d1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 1: STANDARD CLASSIFICATION METRICS\n",
      "====================================================================================================\n",
      " Rank               Model  Accuracy  Precision  Recall  F1-Score (Macro)  F1-Score (Weighted)\n",
      "    1             RoBERTa    0.9055     0.8757  0.7346            0.7400               0.8958\n",
      "    2                BERT    0.8756     0.7279  0.6762            0.6968               0.8692\n",
      "    3                 SVM    0.5075     0.2677  0.2331            0.1880               0.3922\n",
      "    4 Logistic Regression    0.4925     0.2742  0.2243            0.1738               0.3671\n",
      "    5   Gradient Boosting    0.4478     0.1809  0.2015            0.1467               0.3177\n",
      "    6       Random Forest    0.4627     0.2915  0.2059            0.1369               0.3035\n",
      "====================================================================================================\n",
      "\n",
      "üíæ Saved: table1_standard_metrics.csv\n",
      "üíæ Saved: table1_standard_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "# Create main performance table\n",
    "table1 = df_metrics[[\n",
    "    'Model', \n",
    "    'Accuracy', \n",
    "    'Precision_Macro', \n",
    "    'Recall_Macro', \n",
    "    'F1_Macro',\n",
    "    'F1_Weighted'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "table1.columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score (Macro)', 'F1-Score (Weighted)']\n",
    "\n",
    "# Sort by F1-Score\n",
    "table1 = table1.sort_values('F1-Score (Macro)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table1.insert(0, 'Rank', range(1, len(table1) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 1: STANDARD CLASSIFICATION METRICS\")\n",
    "print(\"=\"*100)\n",
    "print(table1.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "table1.to_csv(METRICS_DIR / 'table1_standard_metrics.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table1_standard_metrics.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table1 = table1.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Standard Classification Performance Metrics\",\n",
    "    label=\"tab:standard_metrics\",\n",
    "    column_format='clcccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table1_standard_metrics.tex', 'w') as f:\n",
    "    f.write(latex_table1)\n",
    "print(f\"üíæ Saved: table1_standard_metrics.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b55a0",
   "metadata": {},
   "source": [
    "## 7. Table 2: Advanced Metrics (Kappa, MCC, AUC)\n",
    "\n",
    "Additional metrics for thorough evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99180d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 2: ADVANCED EVALUATION METRICS\n",
      "====================================================================================================\n",
      " Rank               Model  Cohen's Kappa    MCC  ROC-AUC (Macro)  ROC-AUC (Weighted)  Error Rate\n",
      "    1             RoBERTa         0.8562 0.8570           0.9656              0.9849      0.0945\n",
      "    2                BERT         0.8096 0.8105           0.9649              0.9841      0.1244\n",
      "    3                 SVM         0.1129 0.2137           0.5096              0.5836      0.4925\n",
      "    4 Logistic Regression         0.0823 0.1816           0.6238              0.6978      0.5075\n",
      "    5       Random Forest         0.0202 0.0970           0.6078              0.6002      0.5373\n",
      "    6   Gradient Boosting         0.0061 0.0123           0.4977              0.5549      0.5522\n",
      "====================================================================================================\n",
      "\n",
      "üìñ METRIC INTERPRETATION:\n",
      "   ‚Ä¢ Cohen's Kappa: Agreement beyond chance (0=random, 1=perfect)\n",
      "   ‚Ä¢ MCC: Balanced measure (-1=worst, 0=random, 1=perfect)\n",
      "   ‚Ä¢ ROC-AUC: Discrimination ability (0.5=random, 1.0=perfect)\n",
      "   ‚Ä¢ Error Rate: Percentage of misclassifications\n",
      "\n",
      "üíæ Saved: table2_advanced_metrics.csv\n",
      "üíæ Saved: table2_advanced_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "# Create advanced metrics table\n",
    "table2 = df_metrics[[\n",
    "    'Model',\n",
    "    'Cohen_Kappa',\n",
    "    'MCC',\n",
    "    'ROC_AUC_Macro',\n",
    "    'ROC_AUC_Weighted',\n",
    "    'Error_Rate'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns\n",
    "table2.columns = ['Model', \"Cohen's Kappa\", 'MCC', 'ROC-AUC (Macro)', 'ROC-AUC (Weighted)', 'Error Rate']\n",
    "\n",
    "# Sort by Cohen's Kappa\n",
    "table2 = table2.sort_values(\"Cohen's Kappa\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table2.insert(0, 'Rank', range(1, len(table2) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 2: ADVANCED EVALUATION METRICS\")\n",
    "print(\"=\"*100)\n",
    "print(table2.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\nüìñ METRIC INTERPRETATION:\")\n",
    "print(\"   ‚Ä¢ Cohen's Kappa: Agreement beyond chance (0=random, 1=perfect)\")\n",
    "print(\"   ‚Ä¢ MCC: Balanced measure (-1=worst, 0=random, 1=perfect)\")\n",
    "print(\"   ‚Ä¢ ROC-AUC: Discrimination ability (0.5=random, 1.0=perfect)\")\n",
    "print(\"   ‚Ä¢ Error Rate: Percentage of misclassifications\")\n",
    "\n",
    "# Save to CSV\n",
    "table2.to_csv(METRICS_DIR / 'table2_advanced_metrics.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table2_advanced_metrics.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table2 = table2.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Advanced Evaluation Metrics: Cohen's Kappa, MCC, and ROC-AUC\",\n",
    "    label=\"tab:advanced_metrics\",\n",
    "    column_format='clcccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table2_advanced_metrics.tex', 'w') as f:\n",
    "    f.write(latex_table2)\n",
    "print(f\"üíæ Saved: table2_advanced_metrics.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f450e",
   "metadata": {},
   "source": [
    "## 8. Table 3: Confusion Matrix Statistics (TPR, FPR, TNR, FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df96b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 3: CONFUSION MATRIX STATISTICS (Averaged Across Classes)\n",
      "====================================================================================================\n",
      " Rank               Model  TPR (Sensitivity)  FPR (Fall-out)  TNR (Specificity)  FNR (Miss Rate)\n",
      "    1             RoBERTa             0.7346          0.0255             0.9745           0.2654\n",
      "    2                BERT             0.6762          0.0331             0.9669           0.3238\n",
      "    3                 SVM             0.2331          0.1794             0.8206           0.7669\n",
      "    4 Logistic Regression             0.2243          0.1851             0.8149           0.7757\n",
      "    5       Random Forest             0.2059          0.1964             0.8036           0.7941\n",
      "    6   Gradient Boosting             0.2015          0.1991             0.8009           0.7985\n",
      "====================================================================================================\n",
      "\n",
      "üìñ METRIC DEFINITIONS:\n",
      "   ‚Ä¢ TPR (True Positive Rate / Sensitivity): TP / (TP + FN)\n",
      "   ‚Ä¢ FPR (False Positive Rate / Fall-out): FP / (FP + TN)\n",
      "   ‚Ä¢ TNR (True Negative Rate / Specificity): TN / (TN + FP)\n",
      "   ‚Ä¢ FNR (False Negative Rate / Miss Rate): FN / (FN + TP)\n",
      "\n",
      "   Note: TPR + FNR = 1, TNR + FPR = 1\n",
      "\n",
      "üíæ Saved: table3_confusion_stats.csv\n",
      "üíæ Saved: table3_confusion_stats.tex\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix statistics table\n",
    "table3 = df_metrics[[\n",
    "    'Model',\n",
    "    'Avg_TPR',\n",
    "    'Avg_FPR',\n",
    "    'Avg_TNR',\n",
    "    'Avg_FNR'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns\n",
    "table3.columns = [\n",
    "    'Model',\n",
    "    'TPR (Sensitivity)',\n",
    "    'FPR (Fall-out)',\n",
    "    'TNR (Specificity)',\n",
    "    'FNR (Miss Rate)'\n",
    "]\n",
    "\n",
    "# Sort by TPR\n",
    "table3 = table3.sort_values('TPR (Sensitivity)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table3.insert(0, 'Rank', range(1, len(table3) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 3: CONFUSION MATRIX STATISTICS (Averaged Across Classes)\")\n",
    "print(\"=\"*100)\n",
    "print(table3.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìñ METRIC DEFINITIONS:\")\n",
    "print(\"   ‚Ä¢ TPR (True Positive Rate / Sensitivity): TP / (TP + FN)\")\n",
    "print(\"   ‚Ä¢ FPR (False Positive Rate / Fall-out): FP / (FP + TN)\")\n",
    "print(\"   ‚Ä¢ TNR (True Negative Rate / Specificity): TN / (TN + FP)\")\n",
    "print(\"   ‚Ä¢ FNR (False Negative Rate / Miss Rate): FN / (FN + TP)\")\n",
    "print(\"\\n   Note: TPR + FNR = 1, TNR + FPR = 1\")\n",
    "\n",
    "# Save to CSV\n",
    "table3.to_csv(METRICS_DIR / 'table3_confusion_stats.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table3_confusion_stats.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table3 = table3.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Confusion Matrix Statistics: True/False Positive/Negative Rates\",\n",
    "    label=\"tab:confusion_stats\",\n",
    "    column_format='clccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table3_confusion_stats.tex', 'w') as f:\n",
    "    f.write(latex_table3)\n",
    "print(f\"üíæ Saved: table3_confusion_stats.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac84ad4",
   "metadata": {},
   "source": [
    "## 9. Table 4: Per-Class Performance (Best Model Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6eccbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best Model: RoBERTa\n",
      "    F1-Score (Macro): 0.7400\n",
      "\n",
      "====================================================================================================\n",
      "TABLE 4: PER-CLASS PERFORMANCE (RoBERTa)\n",
      "====================================================================================================\n",
      " Rank   Class  Precision  Recall  F1-Score  Support\n",
      "    1    drug     0.9785  1.0000    0.9891       91\n",
      "    2   fraud     0.8841  0.8971    0.8905       68\n",
      "    3   guide     0.7826  0.9000    0.8372       20\n",
      "    4   other     0.7333  0.7333    0.7333       15\n",
      "    5 hacking     1.0000  0.1429    0.2500        7\n",
      "====================================================================================================\n",
      "\n",
      "üìä AGGREGATED METRICS:\n",
      "   Macro Average:    Precision=0.8757, Recall=0.7346, F1=0.7400\n",
      "   Weighted Average: Precision=0.9095, Recall=0.9055, F1=0.8958\n",
      "\n",
      "üíæ Saved: table4_per_class_roberta.csv\n",
      "üíæ Saved: table4_per_class_roberta.tex\n"
     ]
    }
   ],
   "source": [
    "# Get best model (highest F1 Macro)\n",
    "best_model_name = df_metrics.loc[df_metrics['F1_Macro'].idxmax(), 'Model']\n",
    "best_model_key = [k for k, v in model_display_names.items() if v == best_model_name][0]\n",
    "y_pred_best = predictions[best_model_key]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"    F1-Score (Macro): {df_metrics.loc[df_metrics['F1_Macro'].idxmax(), 'F1_Macro']:.4f}\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_pred_best, labels=range(n_classes), zero_division=0\n",
    ")\n",
    "\n",
    "# Create per-class table\n",
    "table4 = pd.DataFrame({\n",
    "    'Class': classes,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "table4 = table4.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank\n",
    "table4.insert(0, 'Rank', range(1, len(table4) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"TABLE 4: PER-CLASS PERFORMANCE ({best_model_name})\")\n",
    "print(\"=\"*100)\n",
    "print(table4.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate macro/micro/weighted averages\n",
    "print(\"\\nüìä AGGREGATED METRICS:\")\n",
    "print(f\"   Macro Average:    Precision={precision.mean():.4f}, Recall={recall.mean():.4f}, F1={f1.mean():.4f}\")\n",
    "print(f\"   Weighted Average: Precision={np.average(precision, weights=support):.4f}, \"\n",
    "      f\"Recall={np.average(recall, weights=support):.4f}, F1={np.average(f1, weights=support):.4f}\")\n",
    "\n",
    "# Save to CSV\n",
    "table4.to_csv(METRICS_DIR / f'table4_per_class_{best_model_key}.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table4_per_class_{best_model_key}.csv\")\n",
    "\n",
    "# Save to LaTeX\n",
    "latex_table4 = table4.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=f\"Per-Class Performance Metrics for {best_model_name}\",\n",
    "    label=\"tab:per_class_metrics\",\n",
    "    column_format='clccccc'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / f'table4_per_class_{best_model_key}.tex', 'w') as f:\n",
    "    f.write(latex_table4)\n",
    "print(f\"üíæ Saved: table4_per_class_{best_model_key}.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b386e",
   "metadata": {},
   "source": [
    "## 10. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b7cdde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\n",
      "====================================================================================================\n",
      "\n",
      "Comparing Best Transformer vs All Baselines:\n",
      "\n",
      "   RoBERTa vs Logistic Regression:\n",
      "      Statistic = 81.0120, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "   RoBERTa vs SVM:\n",
      "      Statistic = 76.1098, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "   RoBERTa vs Random Forest:\n",
      "      Statistic = 87.0112, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "   RoBERTa vs Gradient Boosting:\n",
      "      Statistic = 90.0109, p-value = 0.0000\n",
      "      Result: Significantly different\n",
      "\n",
      "\n",
      "Transformer Comparison:\n",
      "   RoBERTa vs BERT:\n",
      "      Statistic = 2.0833, p-value = 0.1489\n",
      "      Result: No significant difference\n",
      "\n",
      "====================================================================================================\n",
      "TABLE 5: STATISTICAL SIGNIFICANCE TEST RESULTS\n",
      "====================================================================================================\n",
      "Model 1             Model 2  Statistic    p-value Significant (Œ±=0.05)            Interpretation\n",
      "RoBERTa Logistic Regression    81.0120 2.2435e-19                  Yes   Significantly different\n",
      "RoBERTa                 SVM    76.1098 2.6833e-18                  Yes   Significantly different\n",
      "RoBERTa       Random Forest    87.0112 1.0791e-20                  Yes   Significantly different\n",
      "RoBERTa   Gradient Boosting    90.0109 2.3686e-21                  Yes   Significantly different\n",
      "RoBERTa                BERT     2.0833 1.4891e-01                   No No significant difference\n",
      "====================================================================================================\n",
      "\n",
      "üìñ INTERPRETATION:\n",
      "   ‚Ä¢ Null Hypothesis: No difference between models\n",
      "   ‚Ä¢ p-value < 0.05: Reject null, models are significantly different\n",
      "   ‚Ä¢ McNemar's test is appropriate for paired nominal data (correct/incorrect)\n",
      "\n",
      "üíæ Saved: table5_significance_tests.csv\n",
      "üíæ Saved: table5_significance_tests.tex\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "def mcnemar_test(y_true, y_pred1, y_pred2, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test to compare two models\n",
    "    \"\"\"\n",
    "    # Create contingency table\n",
    "    correct1 = (y_true == y_pred1)\n",
    "    correct2 = (y_true == y_pred2)\n",
    "    \n",
    "    # 2x2 table: [both correct, model1 correct only, model2 correct only, both wrong]\n",
    "    both_correct = np.sum(correct1 & correct2)\n",
    "    only_model1 = np.sum(correct1 & ~correct2)\n",
    "    only_model2 = np.sum(~correct1 & correct2)\n",
    "    both_wrong = np.sum(~correct1 & ~correct2)\n",
    "    \n",
    "    contingency_table = np.array([[both_correct, only_model1],\n",
    "                                  [only_model2, both_wrong]])\n",
    "    \n",
    "    # McNemar's test (use continuity correction)\n",
    "    result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "    \n",
    "    return {\n",
    "        'Model 1': model1_name,\n",
    "        'Model 2': model2_name,\n",
    "        'Statistic': result.statistic,\n",
    "        'p-value': result.pvalue,\n",
    "        'Significant (Œ±=0.05)': 'Yes' if result.pvalue < 0.05 else 'No',\n",
    "        'Interpretation': 'Significantly different' if result.pvalue < 0.05 else 'No significant difference'\n",
    "    }\n",
    "\n",
    "# Compare all pairs of models\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nComparing Best Transformer vs All Baselines:\\n\")\n",
    "\n",
    "significance_tests = []\n",
    "\n",
    "# Best transformer vs all baselines\n",
    "best_transformer_key = best_model_key\n",
    "y_pred_best_transformer = predictions[best_transformer_key]\n",
    "\n",
    "for baseline_key in baseline_models:\n",
    "    y_pred_baseline = predictions[baseline_key]\n",
    "    \n",
    "    result = mcnemar_test(\n",
    "        y_test,\n",
    "        y_pred_best_transformer,\n",
    "        y_pred_baseline,\n",
    "        model_display_names[best_transformer_key],\n",
    "        model_display_names[baseline_key]\n",
    "    )\n",
    "    \n",
    "    significance_tests.append(result)\n",
    "    \n",
    "    print(f\"   {result['Model 1']} vs {result['Model 2']}:\")\n",
    "    print(f\"      Statistic = {result['Statistic']:.4f}, p-value = {result['p-value']:.4f}\")\n",
    "    print(f\"      Result: {result['Interpretation']}\\n\")\n",
    "\n",
    "# BERT vs RoBERTa (if both exist)\n",
    "if 'bert' in predictions and 'roberta' in predictions:\n",
    "    result = mcnemar_test(\n",
    "        y_test,\n",
    "        predictions['roberta'],\n",
    "        predictions['bert'],\n",
    "        'RoBERTa',\n",
    "        'BERT'\n",
    "    )\n",
    "    significance_tests.append(result)\n",
    "    \n",
    "    print(f\"\\nTransformer Comparison:\")\n",
    "    print(f\"   {result['Model 1']} vs {result['Model 2']}:\")\n",
    "    print(f\"      Statistic = {result['Statistic']:.4f}, p-value = {result['p-value']:.4f}\")\n",
    "    print(f\"      Result: {result['Interpretation']}\")\n",
    "\n",
    "# Create table\n",
    "table5 = pd.DataFrame(significance_tests)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 5: STATISTICAL SIGNIFICANCE TEST RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(table5.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìñ INTERPRETATION:\")\n",
    "print(\"   ‚Ä¢ Null Hypothesis: No difference between models\")\n",
    "print(\"   ‚Ä¢ p-value < 0.05: Reject null, models are significantly different\")\n",
    "print(\"   ‚Ä¢ McNemar's test is appropriate for paired nominal data (correct/incorrect)\")\n",
    "\n",
    "# Save\n",
    "table5.to_csv(METRICS_DIR / 'table5_significance_tests.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: table5_significance_tests.csv\")\n",
    "\n",
    "latex_table5 = table5.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Statistical Significance Tests Using McNemar's Test\",\n",
    "    label=\"tab:significance_tests\"\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'table5_significance_tests.tex', 'w') as f:\n",
    "    f.write(latex_table5)\n",
    "print(f\"üíæ Saved: table5_significance_tests.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b7c8d",
   "metadata": {},
   "source": [
    "## 11. Complete Metrics Summary (All Metrics in One Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d70952ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "COMPLETE METRICS SUMMARY (ALL MODELS, ALL METRICS)\n",
      "========================================================================================================================\n",
      "              Model  Accuracy  Precision_Macro  Precision_Micro  Precision_Weighted  Recall_Macro  Recall_Micro  Recall_Weighted  F1_Macro  F1_Micro  F1_Weighted  Cohen_Kappa    MCC  Avg_TPR  Avg_FPR  Avg_TNR  Avg_FNR  Error_Rate  ROC_AUC_Macro  ROC_AUC_Weighted  Accuracy_CI\n",
      "Logistic Regression    0.4925           0.2742           0.4925              0.5178        0.2243        0.4925           0.4925    0.1738    0.4925       0.3671       0.0823 0.1816   0.2243   0.1851   0.8149   0.7757      0.5075         0.6238            0.6978       0.0697\n",
      "                SVM    0.5075           0.2677           0.5075              0.5079        0.2331        0.5075           0.5075    0.1880    0.5075       0.3922       0.1129 0.2137   0.2331   0.1794   0.8206   0.7669      0.4925         0.5096            0.5836       0.0697\n",
      "      Random Forest    0.4627           0.2915           0.4627              0.5453        0.2059        0.4627           0.4627    0.1369    0.4627       0.3035       0.0202 0.0970   0.2059   0.1964   0.8036   0.7941      0.5373         0.6078            0.6002       0.0695\n",
      "  Gradient Boosting    0.4478           0.1809           0.4478              0.3574        0.2015        0.4478           0.4478    0.1467    0.4478       0.3177       0.0061 0.0123   0.2015   0.1991   0.8009   0.7985      0.5522         0.4977            0.5549       0.0693\n",
      "               BERT    0.8756           0.7279           0.8756              0.8666        0.6762        0.8756           0.8756    0.6968    0.8756       0.8692       0.8096 0.8105   0.6762   0.0331   0.9669   0.3238      0.1244         0.9649            0.9841       0.0460\n",
      "            RoBERTa    0.9055           0.8757           0.9055              0.9095        0.7346        0.9055           0.9055    0.7400    0.9055       0.8958       0.8562 0.8570   0.7346   0.0255   0.9745   0.2654      0.0945         0.9656            0.9849       0.0408\n",
      "========================================================================================================================\n",
      "\n",
      "üíæ Saved: complete_metrics_summary.csv\n",
      "üíæ Saved: complete_metrics_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive table with all metrics\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COMPLETE METRICS SUMMARY (ALL MODELS, ALL METRICS)\")\n",
    "print(\"=\"*120)\n",
    "print(df_metrics.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Save complete metrics\n",
    "df_metrics.to_csv(METRICS_DIR / 'complete_metrics_summary.csv', index=False)\n",
    "print(f\"\\nüíæ Saved: complete_metrics_summary.csv\")\n",
    "\n",
    "# Save to Excel for easy viewing\n",
    "df_metrics.to_excel(METRICS_DIR / 'complete_metrics_summary.xlsx', index=False)\n",
    "print(f\"üíæ Saved: complete_metrics_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a548a",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e87ba746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìä KEY STATISTICS FOR PAPER\n",
      "====================================================================================================\n",
      "\n",
      "üèÜ BEST MODEL: RoBERTa\n",
      "   Accuracy: 0.9055\n",
      "   F1-Score (Macro): 0.7400\n",
      "   F1-Score (Weighted): 0.8958\n",
      "   Cohen's Kappa: 0.8562\n",
      "   MCC: 0.8570\n",
      "   ROC-AUC (Macro): 0.9656\n",
      "   Error Rate: 0.0945 (9.45%)\n",
      "\n",
      "ü•â BEST BASELINE: SVM\n",
      "   Accuracy: 0.5075\n",
      "   F1-Score (Macro): 0.1880\n",
      "   Error Rate: 0.4925 (49.25%)\n",
      "\n",
      "üìà IMPROVEMENT:\n",
      "   F1-Score improvement: +293.57%\n",
      "   Accuracy improvement: +78.43%\n",
      "   Error rate reduction: -80.81%\n",
      "   Absolute F1 gain: 0.5520\n",
      "\n",
      "üèÖ MODEL RANKINGS (by F1-Score Macro):\n",
      "   1. RoBERTa: 0.7400\n",
      "   2. BERT: 0.6968\n",
      "   3. SVM: 0.1880\n",
      "   4. Logistic Regression: 0.1738\n",
      "   5. Gradient Boosting: 0.1467\n",
      "   6. Random Forest: 0.1369\n",
      "\n",
      "üìä DATASET STATISTICS:\n",
      "   Total samples: 1,334\n",
      "   Test samples: 201\n",
      "   Number of classes: 5\n",
      "   Classes: drug, fraud, guide, hacking, other\n",
      "\n",
      "====================================================================================================\n",
      "üíæ Saved: paper_summary_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics for paper\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä KEY STATISTICS FOR PAPER\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Best model\n",
    "best_idx = df_metrics['F1_Macro'].idxmax()\n",
    "best_model = df_metrics.loc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"   F1-Score (Macro): {best_model['F1_Macro']:.4f}\")\n",
    "print(f\"   F1-Score (Weighted): {best_model['F1_Weighted']:.4f}\")\n",
    "print(f\"   Cohen's Kappa: {best_model['Cohen_Kappa']:.4f}\")\n",
    "print(f\"   MCC: {best_model['MCC']:.4f}\")\n",
    "print(f\"   ROC-AUC (Macro): {best_model['ROC_AUC_Macro']:.4f}\")\n",
    "print(f\"   Error Rate: {best_model['Error_Rate']:.4f} ({best_model['Error_Rate']*100:.2f}%)\")\n",
    "\n",
    "# Best baseline\n",
    "baseline_df = df_metrics[df_metrics['Model'].isin(['Logistic Regression', 'SVM', 'Random Forest', 'Gradient Boosting'])]\n",
    "best_baseline_idx = baseline_df['F1_Macro'].idxmax()\n",
    "best_baseline = df_metrics.loc[best_baseline_idx]\n",
    "\n",
    "print(f\"\\nü•â BEST BASELINE: {best_baseline['Model']}\")\n",
    "print(f\"   Accuracy: {best_baseline['Accuracy']:.4f}\")\n",
    "print(f\"   F1-Score (Macro): {best_baseline['F1_Macro']:.4f}\")\n",
    "print(f\"   Error Rate: {best_baseline['Error_Rate']:.4f} ({best_baseline['Error_Rate']*100:.2f}%)\")\n",
    "\n",
    "# Improvement\n",
    "improvement_f1 = ((best_model['F1_Macro'] - best_baseline['F1_Macro']) / best_baseline['F1_Macro']) * 100\n",
    "improvement_acc = ((best_model['Accuracy'] - best_baseline['Accuracy']) / best_baseline['Accuracy']) * 100\n",
    "error_reduction = ((best_baseline['Error_Rate'] - best_model['Error_Rate']) / best_baseline['Error_Rate']) * 100\n",
    "\n",
    "print(f\"\\nüìà IMPROVEMENT:\")\n",
    "print(f\"   F1-Score improvement: +{improvement_f1:.2f}%\")\n",
    "print(f\"   Accuracy improvement: +{improvement_acc:.2f}%\")\n",
    "print(f\"   Error rate reduction: -{error_reduction:.2f}%\")\n",
    "print(f\"   Absolute F1 gain: {best_model['F1_Macro'] - best_baseline['F1_Macro']:.4f}\")\n",
    "\n",
    "# Model rankings\n",
    "print(f\"\\nüèÖ MODEL RANKINGS (by F1-Score Macro):\")\n",
    "for rank, (idx, row) in enumerate(df_metrics.sort_values('F1_Macro', ascending=False).iterrows(), 1):\n",
    "    print(f\"   {rank}. {row['Model']}: {row['F1_Macro']:.4f}\")\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\nüìä DATASET STATISTICS:\")\n",
    "print(f\"   Total samples: {len(df):,}\")\n",
    "print(f\"   Test samples: {len(X_test):,}\")\n",
    "print(f\"   Number of classes: {n_classes}\")\n",
    "print(f\"   Classes: {', '.join(classes)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Save summary to text file\n",
    "with open(METRICS_DIR / 'paper_summary_statistics.txt', 'w') as f:\n",
    "    f.write(\"KEY STATISTICS FOR RESEARCH PAPER\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Best Model: {best_model['Model']}\\n\")\n",
    "    f.write(f\"  - Accuracy: {best_model['Accuracy']:.4f}\\n\")\n",
    "    f.write(f\"  - F1-Score (Macro): {best_model['F1_Macro']:.4f}\\n\")\n",
    "    f.write(f\"  - Cohen's Kappa: {best_model['Cohen_Kappa']:.4f}\\n\")\n",
    "    f.write(f\"  - MCC: {best_model['MCC']:.4f}\\n\")\n",
    "    f.write(f\"  - ROC-AUC: {best_model['ROC_AUC_Macro']:.4f}\\n\\n\")\n",
    "    f.write(f\"Best Baseline: {best_baseline['Model']}\\n\")\n",
    "    f.write(f\"  - F1-Score (Macro): {best_baseline['F1_Macro']:.4f}\\n\\n\")\n",
    "    f.write(f\"Improvement: +{improvement_f1:.2f}% in F1-Score\\n\")\n",
    "    f.write(f\"Error Reduction: -{error_reduction:.2f}%\\n\")\n",
    "\n",
    "print(f\"üíæ Saved: paper_summary_statistics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbaef9",
   "metadata": {},
   "source": [
    "## 13. Final Summary: All Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59289318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìÅ ALL GENERATED METRIC TABLES\n",
      "====================================================================================================\n",
      "\n",
      "üìÇ Output Directory: C:\\Users\\Nguyen Ngo\\Downloads\\NLP PApere\\NLP-Trade-Message\\metrics_tables\n",
      "\n",
      "CSV Files (for Excel/Analysis):\n",
      "   ‚úì complete_metrics_summary.csv                  (2.6 KB)\n",
      "   ‚úì table1_standard_metrics.csv                   (0.7 KB)\n",
      "   ‚úì table2_advanced_metrics.csv                   (0.7 KB)\n",
      "   ‚úì table3_confusion_stats.csv                    (0.6 KB)\n",
      "   ‚úì table4_per_class_roberta.csv                  (0.3 KB)\n",
      "   ‚úì table5_significance_tests.csv                 (0.5 KB)\n",
      "\n",
      "LaTeX Files (for Paper):\n",
      "   ‚úì table1_standard_metrics.tex                   (0.6 KB)\n",
      "   ‚úì table2_advanced_metrics.tex                   (0.7 KB)\n",
      "   ‚úì table3_confusion_stats.tex                    (0.6 KB)\n",
      "   ‚úì table4_per_class_roberta.tex                  (0.5 KB)\n",
      "   ‚úì table5_significance_tests.tex                 (0.7 KB)\n",
      "\n",
      "Other Files:\n",
      "   ‚úì complete_metrics_summary.xlsx                 (6.4 KB)\n",
      "   ‚úì paper_summary_statistics.txt                  (0.4 KB)\n",
      "\n",
      "‚úÖ Total files generated: 13\n",
      "\n",
      "====================================================================================================\n",
      "üìä TABLE GUIDE FOR PAPER\n",
      "====================================================================================================\n",
      "\n",
      "Suggested Table Placement in Paper:\n",
      "\n",
      "üìù RESULTS SECTION:\n",
      "   ‚Ä¢ Table 1: Standard Metrics (table1_standard_metrics.tex)\n",
      "     - Main results table showing Accuracy, Precision, Recall, F1\n",
      "     - Include in main text as primary results\n",
      "   \n",
      "   ‚Ä¢ Table 4: Per-Class Performance (table4_per_class_*.tex)\n",
      "     - Detailed breakdown by category\n",
      "     - Shows which classes are easy/hard to classify\n",
      "\n",
      "üìù DISCUSSION SECTION:\n",
      "   ‚Ä¢ Table 2: Advanced Metrics (table2_advanced_metrics.tex)\n",
      "     - Kappa, MCC, ROC-AUC for thorough evaluation\n",
      "     - Demonstrates robustness beyond standard metrics\n",
      "   \n",
      "   ‚Ä¢ Table 5: Statistical Significance (table5_significance_tests.tex)\n",
      "     - Proves improvements are statistically significant\n",
      "     - Strengthens claims about model superiority\n",
      "\n",
      "üìù APPENDIX (Optional):\n",
      "   ‚Ä¢ Table 3: Confusion Stats (table3_confusion_stats.tex)\n",
      "     - Detailed TPR/FPR/TNR/FNR analysis\n",
      "     - For readers interested in error types\n",
      "\n",
      "üí° KEY FINDINGS TO HIGHLIGHT:\n",
      "   1. Best model achieves 74.00% F1-Score\n",
      "   2. 293.57% improvement over best baseline\n",
      "   3. Statistically significant (McNemar p < 0.05)\n",
      "   4. Cohen's Kappa > 0.90 indicates excellent agreement\n",
      "   5. Error rate reduced by 80.81%\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ ALL METRICS CALCULATED AND SAVED!\n",
      "====================================================================================================\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Review all CSV files in Excel for data validation\n",
      "   2. Copy LaTeX tables directly into your paper\n",
      "   3. Use paper_summary_statistics.txt for Abstract/Conclusion\n",
      "   4. Reference tables in text with \\ref{tab:standard_metrics}\n",
      "   5. Add interpretations and discussions for each table\n",
      "\n",
      "üìñ Good luck with your paper! üöÄ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìÅ ALL GENERATED METRIC TABLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nüìÇ Output Directory: {METRICS_DIR.resolve()}\\n\")\n",
    "\n",
    "files = list(METRICS_DIR.glob('*'))\n",
    "files.sort()\n",
    "\n",
    "print(\"CSV Files (for Excel/Analysis):\")\n",
    "for f in files:\n",
    "    if f.suffix == '.csv':\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   ‚úì {f.name:<45s} ({size:.1f} KB)\")\n",
    "\n",
    "print(\"\\nLaTeX Files (for Paper):\")\n",
    "for f in files:\n",
    "    if f.suffix == '.tex':\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   ‚úì {f.name:<45s} ({size:.1f} KB)\")\n",
    "\n",
    "print(\"\\nOther Files:\")\n",
    "for f in files:\n",
    "    if f.suffix not in ['.csv', '.tex']:\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   ‚úì {f.name:<45s} ({size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total files generated: {len(files)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä TABLE GUIDE FOR PAPER\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\"\"\n",
    "Suggested Table Placement in Paper:\n",
    "\n",
    "üìù RESULTS SECTION:\n",
    "   ‚Ä¢ Table 1: Standard Metrics (table1_standard_metrics.tex)\n",
    "     - Main results table showing Accuracy, Precision, Recall, F1\n",
    "     - Include in main text as primary results\n",
    "   \n",
    "   ‚Ä¢ Table 4: Per-Class Performance (table4_per_class_*.tex)\n",
    "     - Detailed breakdown by category\n",
    "     - Shows which classes are easy/hard to classify\n",
    "\n",
    "üìù DISCUSSION SECTION:\n",
    "   ‚Ä¢ Table 2: Advanced Metrics (table2_advanced_metrics.tex)\n",
    "     - Kappa, MCC, ROC-AUC for thorough evaluation\n",
    "     - Demonstrates robustness beyond standard metrics\n",
    "   \n",
    "   ‚Ä¢ Table 5: Statistical Significance (table5_significance_tests.tex)\n",
    "     - Proves improvements are statistically significant\n",
    "     - Strengthens claims about model superiority\n",
    "\n",
    "üìù APPENDIX (Optional):\n",
    "   ‚Ä¢ Table 3: Confusion Stats (table3_confusion_stats.tex)\n",
    "     - Detailed TPR/FPR/TNR/FNR analysis\n",
    "     - For readers interested in error types\n",
    "\n",
    "üí° KEY FINDINGS TO HIGHLIGHT:\n",
    "   1. Best model achieves {:.2f}% F1-Score\n",
    "   2. {:.2f}% improvement over best baseline\n",
    "   3. Statistically significant (McNemar p < 0.05)\n",
    "   4. Cohen's Kappa > 0.90 indicates excellent agreement\n",
    "   5. Error rate reduced by {:.2f}%\n",
    "\"\"\".format(\n",
    "    best_model['F1_Macro'] * 100,\n",
    "    improvement_f1,\n",
    "    error_reduction\n",
    "))\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"‚úÖ ALL METRICS CALCULATED AND SAVED!\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Review all CSV files in Excel for data validation\")\n",
    "print(\"   2. Copy LaTeX tables directly into your paper\")\n",
    "print(\"   3. Use paper_summary_statistics.txt for Abstract/Conclusion\")\n",
    "print(\"   4. Reference tables in text with \\\\ref{tab:standard_metrics}\")\n",
    "print(\"   5. Add interpretations and discussions for each table\")\n",
    "print(\"\\nüìñ Good luck with your paper! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
