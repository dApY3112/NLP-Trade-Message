{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e27ee9",
   "metadata": {},
   "source": [
    "# Transformer Models (Clean Runner)\n",
    "\n",
    "This notebook is a streamlined version of `advanced_model_bert.ipynb` for paper-ready experiments:\n",
    "- Loads `data/english_clean.csv`\n",
    "- Builds train/val/test splits\n",
    "- Defines metrics\n",
    "- Runs multi-model fine-tuning (BERT/RoBERTa/XLM-RoBERTa) with optional multi-seed\n",
    "- Writes per-run JSON + a summary CSV to `models/bert_models/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e039e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è accelerate: 1.12.0\n",
      "üêç Python: c:\\Anaconda\\envs\\py310\\python.exe\n",
      "üñ•Ô∏è Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import json\n",
    "import pickle\n",
    "import csv\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ensure accelerate is available BEFORE importing/using Trainer/TrainingArguments\n",
    "def _ensure_accelerate():\n",
    "    try:\n",
    "        import accelerate\n",
    "        return accelerate\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"accelerate>=0.26.0\"])\n",
    "        importlib.invalidate_caches()\n",
    "        import accelerate\n",
    "        return accelerate\n",
    "\n",
    "accelerate = _ensure_accelerate()\n",
    "print(\"‚öôÔ∏è accelerate:\", getattr(accelerate, \"__version__\", \"unknown\"))\n",
    "\n",
    "# Defensive reload: if transformers was imported before accelerate was installed, or if a previous import was partial,\n",
    "# modules may be missing required symbols in the current kernel session.\n",
    "import transformers\n",
    "import transformers.training_args as _training_args_mod\n",
    "import transformers.trainer as _trainer_mod\n",
    "import transformers.modeling_utils as _modeling_utils_mod\n",
    "\n",
    "_needs_reload = (\n",
    "    (not hasattr(_training_args_mod, \"AcceleratorConfig\"))\n",
    "    or (not hasattr(_trainer_mod, \"DataLoaderConfiguration\"))\n",
    "    or (not hasattr(_modeling_utils_mod, \"extract_model_from_parallel\"))\n",
    " )\n",
    "if _needs_reload:\n",
    "    import transformers.utils.import_utils as _import_utils\n",
    "    importlib.reload(_import_utils)\n",
    "    importlib.reload(_training_args_mod)\n",
    "    importlib.reload(_trainer_mod)\n",
    "    importlib.reload(_modeling_utils_mod)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Import these directly from their defining modules to avoid stale references if transformers modules were reloaded.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('üêç Python:', sys.executable)\n",
    "print('üñ•Ô∏è Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3296a1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Using text column: combined_text\n",
      "‚úÖ Loaded rows: 1334\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>root_category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA FULLZ + DL + BACKGROUND REPORT MONTANA | N...</td>\n",
       "      <td>http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>SSN/DOB/DL/PII</td>\n",
       "      <td>fraud</td>\n",
       "      <td>USA FULLZ + DL + BACKGROUND REPORT MONTANA | N...</td>\n",
       "      <td>usa fullz dl background report montana nemesis...</td>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-11T13:00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 x POWER PLANT XL autoflower seed | Nemesis M...</td>\n",
       "      <td>http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Cannabis</td>\n",
       "      <td>drug</td>\n",
       "      <td>1 x POWER PLANT XL autoflower seed | Nemesis M...</td>\n",
       "      <td>1 x power plant xl autoflower seed nemesis mar...</td>\n",
       "      <td>189</td>\n",
       "      <td>2023-01-11T13:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ship Marijuana Safely - Instant Delivery | Nem...</td>\n",
       "      <td>http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Guides and Tutorials</td>\n",
       "      <td>guide</td>\n",
       "      <td>Ship Marijuana Safely - Instant Delivery | Nem...</td>\n",
       "      <td>ship marijuana safely instant delivery nemesis...</td>\n",
       "      <td>142</td>\n",
       "      <td>2023-01-11T12:59:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1x Feminized AUTOFLOWER AK-47 Cannabis Seeds |...</td>\n",
       "      <td>http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Cannabis</td>\n",
       "      <td>drug</td>\n",
       "      <td>1x Feminized AUTOFLOWER AK-47 Cannabis Seeds |...</td>\n",
       "      <td>1x feminized autoflower ak 47 cannabis seeds n...</td>\n",
       "      <td>178</td>\n",
       "      <td>2023-01-11T12:59:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 x KALASHNIKOV autoflower seed | Nemesis Market</td>\n",
       "      <td>http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Cannabis</td>\n",
       "      <td>drug</td>\n",
       "      <td>1 x KALASHNIKOV autoflower seed | Nemesis Mark...</td>\n",
       "      <td>1 x kalashnikov autoflower seed nemesis market...</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-01-11T12:58:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  USA FULLZ + DL + BACKGROUND REPORT MONTANA | N...   \n",
       "1  1 x POWER PLANT XL autoflower seed | Nemesis M...   \n",
       "2  Ship Marijuana Safely - Instant Delivery | Nem...   \n",
       "3  1x Feminized AUTOFLOWER AK-47 Cannabis Seeds |...   \n",
       "4   1 x KALASHNIKOV autoflower seed | Nemesis Market   \n",
       "\n",
       "                                                 url root_category  \\\n",
       "0  http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...         Fraud   \n",
       "1  http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...         Drugs   \n",
       "2  http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...         Other   \n",
       "3  http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...         Drugs   \n",
       "4  http://nemesis555nchzn2dogee6mlc7xxgeeshqirmh3...         Drugs   \n",
       "\n",
       "            subcategory  label  \\\n",
       "0        SSN/DOB/DL/PII  fraud   \n",
       "1              Cannabis   drug   \n",
       "2  Guides and Tutorials  guide   \n",
       "3              Cannabis   drug   \n",
       "4              Cannabis   drug   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  USA FULLZ + DL + BACKGROUND REPORT MONTANA | N...   \n",
       "1  1 x POWER PLANT XL autoflower seed | Nemesis M...   \n",
       "2  Ship Marijuana Safely - Instant Delivery | Nem...   \n",
       "3  1x Feminized AUTOFLOWER AK-47 Cannabis Seeds |...   \n",
       "4  1 x KALASHNIKOV autoflower seed | Nemesis Mark...   \n",
       "\n",
       "                                          clean_text  word_count  \\\n",
       "0  usa fullz dl background report montana nemesis...          23   \n",
       "1  1 x power plant xl autoflower seed nemesis mar...         189   \n",
       "2  ship marijuana safely instant delivery nemesis...         142   \n",
       "3  1x feminized autoflower ak 47 cannabis seeds n...         178   \n",
       "4  1 x kalashnikov autoflower seed nemesis market...         170   \n",
       "\n",
       "             timestamp  \n",
       "0  2023-01-11T13:00:52  \n",
       "1  2023-01-11T13:00:01  \n",
       "2  2023-01-11T12:59:34  \n",
       "3  2023-01-11T12:59:08  \n",
       "4  2023-01-11T12:58:53  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths + load data\n",
    "DATA_DIR = Path('./data')\n",
    "MODELS_DIR = Path('./models')\n",
    "BERT_MODELS_DIR = MODELS_DIR / 'bert_models'\n",
    "BERT_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_FILE = DATA_DIR / 'english_clean.csv'\n",
    "assert INPUT_FILE.exists(), f'Missing file: {INPUT_FILE.resolve()}. Run labeling_and_preprocessing.ipynb first.'\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df = df.dropna(subset=['clean_text', 'label']).copy()\n",
    "\n",
    "text_column = 'combined_text' if 'combined_text' in df.columns else 'clean_text'\n",
    "print('üìù Using text column:', text_column)\n",
    "print('‚úÖ Loaded rows:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3d42b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä num_labels: 5\n",
      "üíæ Saved: models/bert_models/label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print('üìä num_labels:', num_labels)\n",
    "\n",
    "with open(BERT_MODELS_DIR / 'label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print('üíæ Saved:', (BERT_MODELS_DIR / 'label_encoder.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1badc12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 933\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 201\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Train/Val/Test split + HF DatasetDict\n",
    "X = df[text_column].astype(str).values\n",
    "y = df['label_encoded'].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': X_train.tolist(), 'label': y_train.tolist()}),\n",
    "    'validation': Dataset.from_dict({'text': X_val.tolist(), 'label': y_val.tolist()}),\n",
    "    'test': Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()}),\n",
    "})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b5848ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MAX_LENGTH (auto): 303\n",
      "üí° Tip: for lightweight training, pass max_length=128 in the runner.\n",
      "ü™∂ Lightweight models: ('DistilBERT', 'DistilRoBERTa', 'TinyBERT', 'MiniBERT', 'SmallBERT', 'ELECTRA-Small')\n"
     ]
    }
   ],
   "source": [
    "# Model checkpoints + MAX_LENGTH\n",
    "MODEL_CHECKPOINTS = {\n",
    "    # Paper baselines (heavier)\n",
    "    'BERT': 'bert-base-uncased',\n",
    "    'RoBERTa': 'roberta-base',\n",
    "    'XLM-RoBERTa': 'xlm-roberta-base',\n",
    "    \n",
    "    # Lightweight options (faster / less VRAM)\n",
    "    'DistilBERT': 'distilbert-base-uncased',\n",
    "    'DistilRoBERTa': 'distilroberta-base',\n",
    "    'TinyBERT': 'prajjwal1/bert-tiny',\n",
    "    'MiniBERT': 'prajjwal1/bert-mini',\n",
    "    'SmallBERT': 'prajjwal1/bert-small',\n",
    "    'ELECTRA-Small': 'google/electra-small-discriminator',\n",
    "}\n",
    "\n",
    "LIGHTWEIGHT_MODEL_NAMES = (\n",
    "    'DistilBERT',\n",
    "    'DistilRoBERTa',\n",
    "    'TinyBERT',\n",
    "    'MiniBERT',\n",
    "    'SmallBERT',\n",
    "    'ELECTRA-Small',\n",
    ")\n",
    "\n",
    "# Pick a tokenizer just to estimate token lengths (we'll re-tokenize per model during training)\n",
    "_probe_tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINTS['DistilBERT'])\n",
    "text_lengths = [len(_probe_tokenizer.encode(t, truncation=True, max_length=512)) for t in X_train[:500]]\n",
    "MAX_LENGTH = min(int(np.percentile(text_lengths, 95)), 512)\n",
    "print('üéØ MAX_LENGTH (auto):', MAX_LENGTH)\n",
    "print(\"üí° Tip: for lightweight training, pass max_length=128 in the runner.\")\n",
    "print('ü™∂ Lightweight models:', LIGHTWEIGHT_MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f5dddfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ compute_metrics ready\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "print('‚úÖ compute_metrics ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36528a15",
   "metadata": {},
   "source": [
    "## Multi-model fine-tuning\n",
    "Uncomment the last line to run. Results go to `models/bert_models/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b16cc57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Fine-tuning: DistilBERT\n",
      "   checkpoint=distilbert-base-uncased\n",
      "   seed=42\n",
      "   max_length=128\n",
      "   output=distilbert_seed42_1769876624_finetuned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (DistilBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:00<00:00, 3141.78 examples/s]\n",
      "Tokenizing (DistilBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 4672.30 examples/s]\n",
      "Tokenizing (DistilBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 5418.16 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 01:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.630300</td>\n",
       "      <td>0.536908</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.351180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: distilbert_seed42_1769876624\n",
      "\n",
      "üöÄ Fine-tuning: DistilRoBERTa\n",
      "   checkpoint=distilroberta-base\n",
      "   seed=42\n",
      "   max_length=128\n",
      "   output=distilroberta_seed42_1769876756_finetuned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (DistilRoBERTa): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:00<00:00, 3835.68 examples/s]\n",
      "Tokenizing (DistilRoBERTa): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 3749.65 examples/s]\n",
      "Tokenizing (DistilRoBERTa): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 7191.28 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 01:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.563200</td>\n",
       "      <td>0.457039</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.690499</td>\n",
       "      <td>0.465630</td>\n",
       "      <td>0.471487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: distilroberta_seed42_1769876756\n",
      "\n",
      "üöÄ Fine-tuning: TinyBERT\n",
      "   checkpoint=prajjwal1/bert-tiny\n",
      "   seed=42\n",
      "   max_length=128\n",
      "   output=tinybert_seed42_1769876917_finetuned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (TinyBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:00<00:00, 5633.84 examples/s]\n",
      "Tokenizing (TinyBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 5239.47 examples/s]\n",
      "Tokenizing (TinyBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 6139.08 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.511700</td>\n",
       "      <td>1.477178</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.257537</td>\n",
       "      <td>0.313995</td>\n",
       "      <td>0.279309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: tinybert_seed42_1769876917\n",
      "\n",
      "üöÄ Fine-tuning: MiniBERT\n",
      "   checkpoint=prajjwal1/bert-mini\n",
      "   seed=42\n",
      "   max_length=128\n",
      "   output=minibert_seed42_1769876927_finetuned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (MiniBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:00<00:00, 6294.78 examples/s]\n",
      "Tokenizing (MiniBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 4711.14 examples/s]\n",
      "Tokenizing (MiniBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 6322.74 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.271100</td>\n",
       "      <td>1.226802</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.230142</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.170466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: minibert_seed42_1769876927\n",
      "\n",
      "üöÄ Fine-tuning: SmallBERT\n",
      "   checkpoint=prajjwal1/bert-small\n",
      "   seed=42\n",
      "   max_length=128\n",
      "   output=smallbert_seed42_1769876949_finetuned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (SmallBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:00<00:00, 6575.29 examples/s]\n",
      "Tokenizing (SmallBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 5414.56 examples/s]\n",
      "Tokenizing (SmallBERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 5944.67 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>0.671602</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.319726</td>\n",
       "      <td>0.394861</td>\n",
       "      <td>0.348994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: smallbert_seed42_1769876949\n",
      "\n",
      "üöÄ Fine-tuning: ELECTRA-Small\n",
      "   checkpoint=google/electra-small-discriminator\n",
      "   seed=42\n",
      "   max_length=128\n",
      "   output=electra_small_seed42_1769877226_finetuned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (ELECTRA-Small): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:00<00:00, 3387.32 examples/s]\n",
      "Tokenizing (ELECTRA-Small): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 4373.53 examples/s]\n",
      "Tokenizing (ELECTRA-Small): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 4838.75 examples/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.438500</td>\n",
       "      <td>1.400171</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: electra_small_seed42_1769877226\n"
     ]
    }
   ],
   "source": [
    "def _model_slug(name: str) -> str:\n",
    "    return (name or '').strip().lower().replace(' ', '_').replace('-', '_').replace('/', '_')\n",
    "\n",
    "def _append_csv_row(csv_path: Path, row: dict) -> None:\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_header = not csv_path.exists()\n",
    "    with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "def finetune_one_transformer(\n",
    "    model_name: str,\n",
    "    *,\n",
    "    seed: int = RANDOM_STATE,\n",
    "    num_epochs: int = 3,\n",
    "    train_batch_size: int = 16,\n",
    "    eval_batch_size: int = 32,\n",
    "    learning_rate: float = 2e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    max_length: int | None = None,\n",
    "    max_train_samples: int | None = None,\n",
    "    max_eval_samples: int | None = None,\n",
    "    ) -> dict:\n",
    "    if model_name not in MODEL_CHECKPOINTS:\n",
    "        raise ValueError(f'Unknown model_name={model_name!r}. Choose one of: {list(MODEL_CHECKPOINTS.keys())}')\n",
    "\n",
    "    checkpoint = MODEL_CHECKPOINTS[model_name]\n",
    "    run_tag = f\"{_model_slug(model_name)}_seed{seed}_{int(time.time())}\"\n",
    "    run_output_dir = BERT_MODELS_DIR / f\"{run_tag}_finetuned\"\n",
    "    final_model_dir = BERT_MODELS_DIR / f\"{run_tag}_final\"\n",
    "    results_file = BERT_MODELS_DIR / f\"{run_tag}_results.json\"\n",
    "    summary_csv = BERT_MODELS_DIR / 'transformer_runs_summary.csv'\n",
    "\n",
    "    run_max_length = int(max_length) if max_length is not None else int(MAX_LENGTH)\n",
    "\n",
    "    print(f\"\\nüöÄ Fine-tuning: {model_name}\\n   checkpoint={checkpoint}\\n   seed={seed}\\n   max_length={run_max_length}\\n   output={run_output_dir.name}\\n\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    # Trainer requires accelerate (already ensured in the imports cell)\n",
    "    try:\n",
    "        import accelerate  # noqa: F401\n",
    "    except ImportError:\n",
    "        raise ImportError(\"accelerate is required. Install with: pip install -U 'accelerate>=0.26.0'\")\n",
    "\n",
    "    def tok_fn(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=run_max_length)\n",
    "\n",
    "    tokenized = dataset_dict.map(tok_fn, batched=True, desc=f'Tokenizing ({model_name})')\n",
    "\n",
    "    if max_train_samples is not None:\n",
    "        tokenized['train'] = tokenized['train'].select(range(min(len(tokenized['train']), int(max_train_samples))))\n",
    "    if max_eval_samples is not None:\n",
    "        tokenized['validation'] = tokenized['validation'].select(range(min(len(tokenized['validation']), int(max_eval_samples))))\n",
    "        tokenized['test'] = tokenized['test'].select(range(min(len(tokenized['test']), int(max_eval_samples))))\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(run_output_dir),\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        logging_dir=str(run_output_dir / 'logs'),\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        seed=seed,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to='none',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized['train'],\n",
    "        eval_dataset=tokenized['validation'],\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    val_results = trainer.evaluate(eval_dataset=tokenized['validation'])\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized['test'])\n",
    "\n",
    "    trainer.save_model(str(final_model_dir))\n",
    "    tokenizer.save_pretrained(str(final_model_dir))\n",
    "\n",
    "    results_dict = {\n",
    "        'run_tag': run_tag,\n",
    "        'model_name': model_name,\n",
    "        'model_checkpoint': checkpoint,\n",
    "        'validation_results': val_results,\n",
    "        'test_results': test_results,\n",
    "        'train_metrics': train_result.metrics,\n",
    "        'training_args': {\n",
    "            'num_epochs': num_epochs,\n",
    "            'train_batch_size': train_batch_size,\n",
    "            'eval_batch_size': eval_batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'max_length': run_max_length,\n",
    "            'seed': seed,\n",
    "            'max_train_samples': max_train_samples,\n",
    "            'max_eval_samples': max_eval_samples,\n",
    "        },\n",
    "        'saved_artifacts': {\n",
    "            'output_dir': str(run_output_dir),\n",
    "            'final_model_dir': str(final_model_dir),\n",
    "        },\n",
    "    }\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "\n",
    "    row = {\n",
    "        'run_tag': run_tag,\n",
    "        'model_name': model_name,\n",
    "        'checkpoint': checkpoint,\n",
    "        'seed': seed,\n",
    "        'eval_accuracy': test_results.get('eval_accuracy'),\n",
    "        'eval_precision': test_results.get('eval_precision'),\n",
    "        'eval_recall': test_results.get('eval_recall'),\n",
    "        'eval_f1': test_results.get('eval_f1'),\n",
    "        'eval_loss': test_results.get('eval_loss'),\n",
    "        'max_length': run_max_length,\n",
    "        'num_epochs': num_epochs,\n",
    "        'train_batch_size': train_batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'final_model_dir': str(final_model_dir),\n",
    "        'results_file': str(results_file),\n",
    "    }\n",
    "    _append_csv_row(summary_csv, row)\n",
    "\n",
    "    del trainer, model, tokenizer, tokenized\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print('‚úÖ Done:', run_tag)\n",
    "    return results_dict\n",
    "\n",
    "def finetune_many_transformers(model_names=('BERT','RoBERTa','XLM-RoBERTa'), seeds=(RANDOM_STATE,), **kwargs):\n",
    "    results = []\n",
    "    for m in model_names:\n",
    "        for s in seeds:\n",
    "            results.append(finetune_one_transformer(m, seed=s, **kwargs))\n",
    "    return results\n",
    "\n",
    "# Lightweight batch run (one shot): all lightweight models, same settings\n",
    "results = finetune_many_transformers(\n",
    "    model_names=LIGHTWEIGHT_MODEL_NAMES,\n",
    "    seeds=(42,),\n",
    "    num_epochs=1,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=16,\n",
    "    max_length=128,\n",
    "    max_train_samples=2000,\n",
    "    max_eval_samples=500,\n",
    ")\n",
    "\n",
    "# Full run (paper):\n",
    "# results = finetune_many_transformers(model_names=('BERT','RoBERTa','XLM-RoBERTa'), seeds=(42,43), num_epochs=3, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecaa06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Fine-tuning: BERT\n",
      "   checkpoint=bert-base-uncased\n",
      "   seed=42\n",
      "   max_length=303\n",
      "   output=bert_seed42_1769878709_finetuned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (BERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933/933 [00:01<00:00, 631.86 examples/s]\n",
      "Tokenizing (BERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 1671.38 examples/s]\n",
      "Tokenizing (BERT): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 1733.70 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/177 00:24 < 1:11:54, 0.04 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = finetune_many_transformers(model_names=('BERT','RoBERTa','XLM-RoBERTa'), seeds=(42,43), num_epochs=3, max_length=MAX_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
